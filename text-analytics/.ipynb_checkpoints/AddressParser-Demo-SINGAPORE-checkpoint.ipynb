{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_file_train_name= 'SG_AddressParsing_v3.csv'\n",
    "\n",
    "batch_size=100 #number of addresses in one batch of training\n",
    "num_epochs= 10 #number of epochs\n",
    "\n",
    "activation=\"Tanh\" #activation function\n",
    "enable_model_save_after_batch=True #whether to save model after each batch\n",
    "class_dic={0:'PlaceName',1:'StreetNumber',2:'StreetName',3:'Suburb',4:'Neighbourhood', 5:'PostCode',6:'Country',7:'UnParsed'}\n",
    "\n",
    "start_batch_num=0\n",
    "\n",
    "# some constants , lists and dictionaries being used\n",
    "\n",
    "reverse_class_dic={'PlaceName':0,'StreetNumber':1,'StreetName':2,'Suburb':3,'Neighbourhood':4,'PostCode':5,'Country':6,'UnParsed':7}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_name2</th>\n",
       "      <th>street_num</th>\n",
       "      <th>street_name</th>\n",
       "      <th>suburb</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>pincode</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guoco Tower</td>\n",
       "      <td>1</td>\n",
       "      <td>Wallich Street</td>\n",
       "      <td>Tanjong Pagar</td>\n",
       "      <td>Chinatown</td>\n",
       "      <td>78881</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URA Centre (East Wing)</td>\n",
       "      <td>1</td>\n",
       "      <td>Ann Siang Road</td>\n",
       "      <td>Tanjong Pagar</td>\n",
       "      <td>Chinatown</td>\n",
       "      <td>69708</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red Dot Design Museum</td>\n",
       "      <td>11</td>\n",
       "      <td>Marina Boulevard</td>\n",
       "      <td>Raffles Place</td>\n",
       "      <td>Chinatown</td>\n",
       "      <td>89140</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red Dot Design Museum</td>\n",
       "      <td>11</td>\n",
       "      <td>Wallich Street</td>\n",
       "      <td>Tanjong Pagar</td>\n",
       "      <td>Chinatown</td>\n",
       "      <td>78881</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guoco Tower</td>\n",
       "      <td>1</td>\n",
       "      <td>Wallich Street</td>\n",
       "      <td>Tanjong Pagar</td>\n",
       "      <td>Chinatown</td>\n",
       "      <td>78881</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>the Jam Bar</td>\n",
       "      <td>13A-15A</td>\n",
       "      <td>South Bridge Road</td>\n",
       "      <td>Raffles Place</td>\n",
       "      <td>Clarke Quay</td>\n",
       "      <td>58657</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>the Jam Bar</td>\n",
       "      <td>13A-15A</td>\n",
       "      <td>South Bridge Road</td>\n",
       "      <td>Raffles Place</td>\n",
       "      <td>Clarke Quay</td>\n",
       "      <td>58657</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>the Jam Bar</td>\n",
       "      <td>11</td>\n",
       "      <td>South Bridge Road</td>\n",
       "      <td>City Hall</td>\n",
       "      <td>Clarke Quay</td>\n",
       "      <td>58716</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Elgin Bridge</td>\n",
       "      <td>11</td>\n",
       "      <td>North Bridge Road</td>\n",
       "      <td>City Hall</td>\n",
       "      <td>Clarke Quay</td>\n",
       "      <td>58716</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Singapore River</td>\n",
       "      <td>11</td>\n",
       "      <td>Read Bridge</td>\n",
       "      <td>City Hall</td>\n",
       "      <td>Clarke Quay</td>\n",
       "      <td>179020</td>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  org_name2 street_num        street_name         suburb  \\\n",
       "0               Guoco Tower          1     Wallich Street  Tanjong Pagar   \n",
       "1    URA Centre (East Wing)          1     Ann Siang Road  Tanjong Pagar   \n",
       "2     Red Dot Design Museum         11   Marina Boulevard  Raffles Place   \n",
       "3     Red Dot Design Museum         11     Wallich Street  Tanjong Pagar   \n",
       "4               Guoco Tower          1     Wallich Street  Tanjong Pagar   \n",
       "..                      ...        ...                ...            ...   \n",
       "105             the Jam Bar    13A-15A  South Bridge Road  Raffles Place   \n",
       "106             the Jam Bar    13A-15A  South Bridge Road  Raffles Place   \n",
       "107             the Jam Bar         11  South Bridge Road      City Hall   \n",
       "108            Elgin Bridge         11  North Bridge Road      City Hall   \n",
       "109         Singapore River         11        Read Bridge      City Hall   \n",
       "\n",
       "    neighbourhood  pincode    country  \n",
       "0       Chinatown    78881  Singapore  \n",
       "1       Chinatown    69708  Singapore  \n",
       "2       Chinatown    89140  Singapore  \n",
       "3       Chinatown    78881  Singapore  \n",
       "4       Chinatown    78881  Singapore  \n",
       "..            ...      ...        ...  \n",
       "105   Clarke Quay    58657  Singapore  \n",
       "106   Clarke Quay    58657  Singapore  \n",
       "107   Clarke Quay    58716  Singapore  \n",
       "108   Clarke Quay    58716  Singapore  \n",
       "109   Clarke Quay   179020  Singapore  \n",
       "\n",
       "[110 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(data_file_train_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(data_file_train_name)['org_name2'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsFromFile(fileName):  ##reads required list\n",
    "        all_words = []\n",
    "        f = open(fileName)\n",
    "        while (1):\n",
    "            line = f.readline().replace('\\n', '')\n",
    "            if line == '':\n",
    "                break\n",
    "            else:\n",
    "                words = line.split('|')\n",
    "            all_words = all_words + words\n",
    "            all_words = ' '.join(all_words).split()\n",
    "        \n",
    "        return list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOOR_WORDS = []\n",
    "ORG_WORDS = []\n",
    "PLACENAME_WORDS = []\n",
    "path = './'\n",
    "FLOOR_WORDS = FLOOR_WORDS + getWordsFromFile(path + 'resources/' + 'unit_types.txt')\n",
    "FLOOR_WORDS = FLOOR_WORDS + getWordsFromFile(path + 'resources/'+ 'level_types.txt')\n",
    "ORG_WORDS = ORG_WORDS + getWordsFromFile( path + 'resources/'+ 'company_types.txt')\n",
    "PLACENAME_WORDS = PLACENAME_WORDS + getWordsFromFile( path+ 'resources/' + 'building_types.txt')\n",
    "PLACENAME_WORDS = PLACENAME_WORDS + getWordsFromFile( path + 'resources/'+ 'place_names.txt')\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index, tags_sen, FLOOR_WORDS, ORG_WORDS, PLACENAME_WORDS):  # feature function\n",
    "\tSTREET_NAMES = [\n",
    "\t\t'allee', 'alley', 'ally', 'aly', 'anex', 'annex', 'annx', 'anx', 'arc',\n",
    "\t\t'arcade', 'av', 'ave', 'aven', 'avenu', 'avenue', 'avn', 'avnue', 'bayoo',\n",
    "\t\t'bayou', 'bch', 'beach', 'bend', 'bg', 'bgs', 'blf', 'blfs', 'bluf',\n",
    "\t\t'bluff', 'bluffs', 'blvd', 'bnd', 'bot', 'bottm', 'bottom', 'boul',\n",
    "\t\t'boulevard', 'boulv', 'br', 'branch', 'brdge', 'brg', 'bridge', 'brk',\n",
    "\t\t'brks', 'brnch', 'brook', 'brooks', 'btm', 'burg', 'burgs', 'byp', 'bypa',\n",
    "\t\t'bypas', 'bypass', 'byps', 'byu', 'camp', 'canyn', 'canyon', 'cape',\n",
    "\t\t'causeway', 'causwa', 'cen', 'cent', 'center', 'centers', 'centr',\n",
    "\t\t'centre', 'cir', 'circ', 'circl', 'circle', 'circles', 'cirs', 'clb',\n",
    "\t\t'clf', 'clfs', 'cliff', 'cliffs', 'club', 'cmn', 'cmns', 'cmp', 'cnter',\n",
    "\t\t'cntr', 'cnyn', 'common', 'commons', 'cor', 'corner', 'corners', 'cors',\n",
    "\t\t'course', 'court', 'courts', 'cove', 'coves', 'cp', 'cpe', 'crcl', 'crcle',\n",
    "\t\t'creek', 'cres', 'crescent', 'crest', 'crk', 'crossing', 'crossroad',\n",
    "\t\t'crossroads', 'crse', 'crsent', 'crsnt', 'crssng', 'crst', 'cswy', 'ct',\n",
    "\t\t'ctr', 'ctrs', 'cts', 'curv', 'curve', 'cv', 'cvs', 'cyn', 'dale', 'dam',\n",
    "\t\t'div', 'divide', 'dl', 'dm', 'dr', 'driv', 'drive', 'drives', 'drs', 'drv',\n",
    "\t\t'dv', 'dvd', 'est', 'estate', 'estates', 'ests', 'exp', 'expr', 'express',\n",
    "\t\t'expressway', 'expw', 'expy', 'ext', 'extension', 'extensions', 'extn',\n",
    "\t\t'extnsn', 'exts', 'fall', 'falls', 'ferry', 'field', 'fields', 'flat',\n",
    "\t\t'flats', 'fld', 'flds', 'fls', 'flt', 'flts', 'ford', 'fords', 'forest',\n",
    "\t\t'forests', 'forg', 'forge', 'forges', 'fork', 'forks', 'fort', 'frd',\n",
    "\t\t'frds', 'freeway', 'freewy', 'frg', 'frgs', 'frk', 'frks', 'frry', 'frst',\n",
    "\t\t'frt', 'frway', 'frwy', 'fry', 'ft', 'fwy', 'garden', 'gardens', 'gardn',\n",
    "\t\t'gateway', 'gatewy', 'gatway', 'gdn', 'gdns', 'glen', 'glens', 'gln',\n",
    "\t\t'glns', 'grden', 'grdn', 'grdns', 'green', 'greens', 'grn', 'grns', 'grov',\n",
    "\t\t'grove', 'groves', 'grv', 'grvs', 'gtway', 'gtwy', 'harb', 'harbor',\n",
    "\t\t'harbors', 'harbr', 'haven', 'hbr', 'hbrs', 'heights', 'highway', 'highwy',\n",
    "\t\t'hill', 'hills', 'hiway', 'hiwy', 'hl', 'hllw', 'hls', 'hollow', 'hollows',\n",
    "\t\t'holw', 'holws', 'hrbor', 'ht', 'hts', 'hvn', 'hway', 'hwy', 'inlet',\n",
    "\t\t'inlt', 'is', 'island', 'islands', 'isle', 'isles', 'islnd', 'islnds',\n",
    "\t\t'iss', 'jct', 'jction', 'jctn', 'jctns', 'jcts', 'junction', 'junctions',\n",
    "\t\t'junctn', 'juncton', 'key', 'keys', 'knl', 'knls', 'knol', 'knoll',\n",
    "\t\t'knolls', 'ky', 'kys', 'lake', 'lakes', 'land', 'landing', 'lane', 'lck',\n",
    "\t\t'lcks', 'ldg', 'ldge', 'lf', 'lgt', 'lgts', 'light', 'lights', 'lk', 'lks',\n",
    "\t\t'ln', 'lndg', 'lndng', 'loaf', 'lock', 'locks', 'lodg', 'lodge', 'loop',\n",
    "\t\t'loops', 'mall', 'manor', 'manors', 'mdw', 'mdws', 'meadow', 'meadows',\n",
    "\t\t'medows', 'mews', 'mill', 'mills', 'mission', 'missn', 'ml', 'mls', 'mnr',\n",
    "\t\t'mnrs', 'mnt', 'mntain', 'mntn', 'mntns', 'motorway', 'mount', 'mountain',\n",
    "\t\t'mountains', 'mountin', 'msn', 'mssn', 'mt', 'mtin', 'mtn', 'mtns', 'mtwy',\n",
    "\t\t'nck', 'neck', 'opas', 'orch', 'orchard', 'orchrd', 'oval', 'overpass',\n",
    "\t\t'ovl', 'park', 'parks', 'parkway', 'parkways', 'parkwy', 'pass', 'passage',\n",
    "\t\t'path', 'paths', 'pike', 'pikes', 'pine', 'pines', 'pkway', 'pkwy',\n",
    "\t\t'pkwys', 'pky', 'pl', 'place', 'plain', 'plains', 'plaza', 'pln', 'plns',\n",
    "\t\t'plz', 'plza', 'pne', 'pnes', 'point', 'points', 'port', 'ports', 'pr',\n",
    "\t\t'prairie', 'prk', 'prr', 'prt', 'prts', 'psge', 'pt', 'pts', 'rad',\n",
    "\t\t'radial', 'radiel', 'radl', 'ramp', 'ranch', 'ranches', 'rapid', 'rapids',\n",
    "\t\t'rd', 'rdg', 'rdge', 'rdgs', 'rds', 'rest', 'ridge', 'ridges', 'riv',\n",
    "\t\t'river', 'rivr', 'rnch', 'rnchs', 'road', 'roads', 'route', 'row', 'rpd',\n",
    "\t\t'rpds', 'rst', 'rte', 'rue', 'run', 'rvr', 'shl', 'shls', 'shoal',\n",
    "\t\t'shoals', 'shoar', 'shoars', 'shore', 'shores', 'shr', 'shrs', 'skwy',\n",
    "\t\t'skyway', 'smt', 'spg', 'spgs', 'spng', 'spngs', 'spring', 'springs',\n",
    "\t\t'sprng', 'sprngs', 'spur', 'spurs', 'sq', 'sqr', 'sqre', 'sqrs', 'sqs',\n",
    "\t\t'squ', 'square', 'squares', 'st', 'sta', 'station', 'statn', 'stn', 'str',\n",
    "\t\t'stra', 'strav', 'straven', 'stravenue', 'stravn', 'stream', 'street',\n",
    "\t\t'streets', 'streme', 'strm', 'strt', 'strvn', 'strvnue', 'sts', 'sumit',\n",
    "\t\t'sumitt', 'summit', 'ter', 'terr', 'terrace', 'throughway', 'tpke',\n",
    "\t\t'trace', 'traces', 'track', 'tracks', 'trafficway', 'trail', 'trailer',\n",
    "\t\t'trails', 'trak', 'trce', 'trfy', 'trk', 'trks', 'trl', 'trlr', 'trlrs',\n",
    "\t\t'trls', 'trnpk', 'trwy', 'tunel', 'tunl', 'tunls', 'tunnel', 'tunnels',\n",
    "\t\t'tunnl', 'turnpike', 'turnpk', 'un', 'underpass', 'union', 'unions', 'uns',\n",
    "\t\t'upas', 'valley', 'valleys', 'vally', 'vdct', 'via', 'viadct', 'viaduct',\n",
    "\t\t'view', 'views', 'vill', 'villag', 'village', 'villages', 'ville', 'villg',\n",
    "\t\t'villiage', 'vis', 'vist', 'vista', 'vl', 'vlg', 'vlgs', 'vlly', 'vly',\n",
    "\t\t'vlys', 'vst', 'vsta', 'vw', 'vws', 'walk', 'walks', 'wall', 'way', 'ways',\n",
    "\t\t'well', 'wells', 'wl', 'wls', 'wy', 'xing', 'xrd', 'xrds']\n",
    "\n",
    "\n",
    "\treturn {\n",
    "\t\t'word': sentence[index],\n",
    "\t\t'street_name': 1 if sentence[index].lower() in STREET_NAMES else 0,\n",
    "\t\t'floorwords': 1 if sentence[index].lower() in FLOOR_WORDS else 0,\n",
    "\t\t'orgwords': 1 if sentence[index].lower() in ORG_WORDS else 0,\n",
    "\t\t'placenamewords': 1 if sentence[index].lower() in PLACENAME_WORDS else 0,\n",
    "\t\t'indandlen': float(index) / len(sentence),\n",
    "\t\t'country': 1 if sentence[index].lower() == 'singapore' else 0,\n",
    "\t\t'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "\t\t'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "\t\t'num_alpha': sum(c.isalpha() for c in sentence[index]),\n",
    "\t\t'num_digit': sum(c.isdigit() for c in sentence[index]),\n",
    "\t}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_append={\n",
    "\t\t'word': 'NA',\n",
    "\t\t'street_name':0,\n",
    "\t\t'floorwords': 0,\n",
    "\t\t'orgwords': 0,\n",
    "\t\t'placenamewords': 0,\n",
    "\t\t'indandlen': 0,\n",
    "\t\t'country': 0,\n",
    "\t\t'prev_word': 'NA',\n",
    "\t\t'next_word': 'NA',\n",
    "\t\t'num_alpha': 0,\n",
    "\t\t'num_digit': 0,\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences,FLOOR_WORDS,ORG_WORDS,PLACENAME_WORDS,num_threshold): #calls to encode feature\n",
    "    X, y = [], []\n",
    "    if(1==1): # no feature analysis\n",
    "        f=open('feature_analysis.txt','w')\n",
    "    for tagged in tagged_sentences:\n",
    "        if(1==1):\n",
    "            f.write(str(tagged)+'\\n')\n",
    "        \n",
    "        for index in range(num_threshold):\n",
    "            if (1==1):\n",
    "                if(index<len(tagged)): \n",
    "                    f.write(str(features(untag(tagged), index, gettag(tagged),FLOOR_WORDS,ORG_WORDS,PLACENAME_WORDS)) + '\\n')\n",
    "            if(index<len(tagged)):           \n",
    "                X.append(features(untag(tagged), index,gettag(tagged),FLOOR_WORDS,ORG_WORDS,PLACENAME_WORDS)) #here cant call get tag . send only those list which have been prediced\n",
    "                y.append(tagged[index][1])\n",
    "            else:\n",
    "                X.append(sample_append)\n",
    "                y.append('UnParsed')\n",
    "    if(1==1):\n",
    "        f.close()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout\n",
    "from keras.datasets import imdb\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importTrainingData(data_file_train_name, startIndex,\n",
    "\t\t\t\t\t   endIndex):# reads data from data file from start index to end that is in batches\n",
    "\t####reads only desired number of samples\n",
    "\tdata_raw = []  ##each row has relevent fileds\n",
    "\tline_counter = 0\n",
    "\tcount_lines_raw_data = 1000000000000000000\n",
    "\tcorrupt_data = 0  # counts corrupt data points\n",
    "\tf = open(data_file_train_name)\n",
    "\twhile (1):\n",
    "\t\ttry:\n",
    "\t\t\tline = f.readline().replace('\"', '').replace('\\n','').rstrip(' ').lstrip(' ')  # delete double quotes\n",
    "\t\t\tif line == '':\n",
    "\t\t\t\tbreak\n",
    "\t\t\telif (line_counter > count_lines_raw_data):\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tline_counter = line_counter + 1\n",
    "\t\t\t\tif (line_counter > startIndex and endIndex > line_counter):  # only extract desired batch\n",
    "\t\t\t\t\tsplit_line = line.split(',')\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tdata_raw.append([split_line[0], split_line[1], split_line[2], split_line[3],\n",
    "\t\t\t\t\t\t\t\t\t split_line[4] , split_line[5], split_line[6] ])\n",
    "\t\texcept IndexError:\n",
    "            \n",
    "\t\t\tcorrupt_data = corrupt_data + 1\n",
    "\n",
    "\t\t\tpass\n",
    "\tf.close()\n",
    "\treturn data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagRawData(data_raw, class_dic):  ##this converts the raw data to brown corpus format\n",
    "\tdata_tagged = []  ##brown corpus format holder\n",
    "\tfor sentence in data_raw:\n",
    "\t\ttuple_list = []  # list of tuples to make for this sentence\n",
    "\t\ttry:\n",
    "\t\t\tfor i in range(0, len(sentence)):\n",
    "\t\t\t\tsub_fields_len = len(sentence[i].split(' '))\n",
    "\t\t\t\tfor j in range(0, sub_fields_len):\n",
    "\t\t\t\t\tif (len(sentence[i].split(' ')[j]) >= 1):  # append only if fields is not empty\n",
    "\t\t\t\t\t\ttuple_list.append((sentence[i].split(' ')[j], class_dic[i]))\n",
    "\t\texcept KeyError:\n",
    "\t\t\tpass\n",
    "\t\tdata_tagged.append(tuple_list)\n",
    "\treturn data_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untag(tagged_sentence):\n",
    "\treturn [w for w, t in tagged_sentence]\n",
    "\n",
    "def gettag(tagged_sentence):  # returns the tag\n",
    "\treturn [t for w, t in tagged_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, Dropout, LSTM, RepeatVector , TimeDistributed, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 20, 8)             23424     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20, 8)             72        \n",
      "=================================================================\n",
      "Total params: 23,496\n",
      "Trainable params: 23,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM  Implementation  Ishaan \n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout\n",
    "from keras.datasets import imdb\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "import keras\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "''' this function describes the training of the data '''\n",
    "#sample_marker=range(0,18000000,\n",
    "#n_total_batch = len(sample_marker) - 1  # total batches to train\n",
    "\n",
    "max_features = 1000\n",
    "model = Sequential()\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(8, input_shape=(20, 723), return_sequences=True))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "start :  0\n",
      "end :  100\n",
      "Importing data\n",
      "Data Loaded\n",
      "1980\n",
      "Data Transformed \n",
      "shape xtransformed :  (1980, 723)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = DictVectorizer(sparse=True)\n",
    "#for index in range(0,100):\n",
    "\n",
    "index = 0 \n",
    "start = 100*index\n",
    "end = start + 100\n",
    "print('===============================================')\n",
    "print('start : ',start)\n",
    "print('end : ', end)\n",
    "print('Importing data')\n",
    "data_raw = importTrainingData(data_file_train_name, start, end)  # import the training Data\n",
    "\n",
    "print('Data Loaded')\n",
    "data_tagged = tagRawData(data_raw, class_dic)  # tag the data\n",
    "X, Y = transform_to_dataset(data_tagged, FLOOR_WORDS, ORG_WORDS,\n",
    "                            PLACENAME_WORDS,20)  # obtain features of the data\n",
    "\n",
    "vectorizer.fit(X)\n",
    "\n",
    "#print(vectorizer.transform(X[0]))\n",
    "\n",
    "print(len(X))\n",
    "print('Data Transformed ')\n",
    "X_train = X[0:]\n",
    "Y_train = Y[0:]\n",
    "\n",
    "new_data_tags=[]\n",
    "for element in Y_train:\n",
    "    new_data_tags.append(reverse_class_dic[element])\n",
    "\n",
    "y_transformed = keras.utils.to_categorical(new_data_tags, 9)\n",
    "\n",
    "\n",
    "\n",
    "X_new_train= X_train\n",
    "\n",
    "X_transformed = vectorizer.transform(X_new_train)\n",
    "\n",
    "print('shape xtransformed : ' , X_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "(1980, 723)\n",
      "<class 'numpy.ndarray'>\n",
      "99\n",
      "(99, 20, 723)\n"
     ]
    }
   ],
   "source": [
    "print(int(X_transformed.shape[0]/20))\n",
    "\n",
    "new_X_transformed = X_transformed.toarray()\n",
    "\n",
    "\n",
    "print(new_X_transformed.shape)\n",
    "print(type(new_X_transformed))\n",
    "DIM1=int(X_transformed.shape[0]/20)\n",
    "print(DIM1)\n",
    "new_X_transformed_reshape = new_X_transformed.reshape(DIM1,20,723)\n",
    "print(new_X_transformed_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vectors\n",
      "Vectors Created\n",
      "Train...\n",
      "<class 'numpy.ndarray'>\n",
      "shape ytransformed :  (1980, 8)\n",
      "(1980, 8)\n",
      "<class 'numpy.ndarray'>\n",
      "(99, 20, 8)\n",
      "(1980, 8)\n"
     ]
    }
   ],
   "source": [
    "print('Creating Vectors')\n",
    "\n",
    "\n",
    "print('Vectors Created')\n",
    "\n",
    "print('Train...') \n",
    "y_transformed = keras.utils.to_categorical(new_data_tags, 8)\n",
    "print(type(y_transformed))\n",
    "print('shape ytransformed : ', y_transformed.shape)\n",
    "\n",
    "print(y_transformed.shape)\n",
    "y_transformed_reshape = y_transformed.reshape(DIM1,20,8)\n",
    "\n",
    "\n",
    "print(type(y_transformed_reshape))\n",
    "print(y_transformed_reshape.shape)\n",
    "print(y_transformed.shape)\n",
    "\n",
    "\n",
    "new_X_transformed_reshape_train = new_X_transformed_reshape[0:1500]\n",
    "new_X_transformed_reshape_test = new_X_transformed_reshape[1500:DIM1]\n",
    "\n",
    "\n",
    "y_transformed_reshape_train = y_transformed_reshape[0:1500]\n",
    "y_transformed_reshape_test = y_transformed_reshape[1500:DIM1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 20, 8)\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 1.8956 - accuracy: 0.3884\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.8845 - accuracy: 0.3985\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.8733 - accuracy: 0.4111\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 1.8618 - accuracy: 0.4146\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.8502 - accuracy: 0.4192\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.8385 - accuracy: 0.4202\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 1.8267 - accuracy: 0.4217\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.8147 - accuracy: 0.4242\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.8026 - accuracy: 0.4303\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 1.7905 - accuracy: 0.4364\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 1.7783 - accuracy: 0.4419\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.7661 - accuracy: 0.4465\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.7538 - accuracy: 0.4535\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.7415 - accuracy: 0.4596\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.7292 - accuracy: 0.4667\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 1.7170 - accuracy: 0.4808\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.7047 - accuracy: 0.4889\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.6925 - accuracy: 0.4985\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 1.6804 - accuracy: 0.5121\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 1.6684 - accuracy: 0.5232\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 1.6564 - accuracy: 0.5419\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.6446 - accuracy: 0.5692\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 1.6329 - accuracy: 0.5919\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 1.6213 - accuracy: 0.6025\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.6099 - accuracy: 0.6172\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.5986 - accuracy: 0.6303\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 1.5874 - accuracy: 0.6399\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.5764 - accuracy: 0.6470\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.5655 - accuracy: 0.6626\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 1.5548 - accuracy: 0.6697\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.5442 - accuracy: 0.6768\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.5338 - accuracy: 0.6823\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 1.5235 - accuracy: 0.6854\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.5133 - accuracy: 0.6879\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1.5033 - accuracy: 0.6904\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 1.4934 - accuracy: 0.6975\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.4837 - accuracy: 0.6970\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.4740 - accuracy: 0.6995\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.4646 - accuracy: 0.7066\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 1.4552 - accuracy: 0.7071\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.4460 - accuracy: 0.7071\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.4370 - accuracy: 0.7071\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.4280 - accuracy: 0.7086\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.4192 - accuracy: 0.7071\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.4105 - accuracy: 0.7076\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.4019 - accuracy: 0.7051\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.3934 - accuracy: 0.7040\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.3851 - accuracy: 0.7051\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.3768 - accuracy: 0.7045\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1.3687 - accuracy: 0.7045\n",
      "Training is Complete !\n"
     ]
    }
   ],
   "source": [
    "print(y_transformed_reshape_train.shape)\n",
    "history = model.fit(new_X_transformed_reshape_train, y_transformed_reshape_train,batch_size=100,epochs=50,verbose=1 ,\n",
    "                   validation_data=(new_X_transformed_reshape_test, y_transformed_reshape_test) )\n",
    "\n",
    "#model.save('lstm_best_val_batch-final'+str(index)+'.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "\n",
    "print('Training is Complete !') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Address Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('../final_trained_model/lstm_best_val_batch-new-49.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parseSingleAddress(inputAddress,model2,vectorizer):\n",
    "    num_threshold = 20\n",
    "    X=[]\n",
    "    y=[]\n",
    "    inputAddress=inputAddress.replace('\"','')\n",
    "    inputAddress=inputAddress.replace(\"'\",\"\")\n",
    "    inputAddress=inputAddress.replace(',',' ')\n",
    "    inputList=inputAddress.split()\n",
    "    tmplis=[]\n",
    "    print(\"Start Parsing\")\n",
    "    data_tagged=[]\n",
    "    for element in inputList:\n",
    "        tmplis.append((element,'NA'))\n",
    "    data_tagged.append(tmplis)\n",
    "    \n",
    " \n",
    "    X_test, Y_test = transform_to_dataset(data_tagged, FLOOR_WORDS, ORG_WORDS,\n",
    "                                    PLACENAME_WORDS,20)  # obtain features of the data\n",
    "    #X_test= check_top_k(X_test,feat_keys)\n",
    "    \n",
    "    X_test = X_test\n",
    "    print(Y_test)\n",
    "    X_transformed_test=[]\n",
    "    X_transformed_test = vectorizer.transform(X_test)\n",
    "\n",
    "    new_X_transformed_test = X_transformed_test.toarray()   \n",
    "    #print('Testing...')\n",
    "    #print(type(new_X_transformed_test))\n",
    "    #print(new_X_transformed_test.shape)\n",
    "    new_X_transformed_test_reshape=new_X_transformed_test.reshape(1,20,723)\n",
    "    prediction = model2.predict_classes(new_X_transformed_test_reshape,batch_size=None, verbose=1)\n",
    "    #print(\"Prediction : \", prediction)\n",
    "    predictionvalue=[]\n",
    "    for i in range(0,len(prediction[0])):\n",
    "        predictionvalue.append(class_dic[prediction[0][i]])\n",
    "    print(\"Address : \",inputAddress)\n",
    "    for j in range(len(inputList)):\n",
    "        print(inputList[j],\": \",predictionvalue[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Parsing\n",
      "['NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed']\n",
      "1/1 [==============================] - 1s 986ms/step\n",
      "Address :  Wallich Street Tanjong Pagar Chinatown 78881 Singapore\n",
      "Wallich :  StreetName\n",
      "Street :  StreetName\n",
      "Tanjong :  Suburb\n",
      "Pagar :  Suburb\n",
      "Chinatown :  Suburb\n",
      "78881 :  UnParsed\n",
      "Singapore :  UnParsed\n"
     ]
    }
   ],
   "source": [
    "address = 'Wallich Street Tanjong Pagar Chinatown 78881 Singapore'\n",
    "parseSingleAddress(address, model , vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Parsing\n",
      "['NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed', 'UnParsed']\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Address :  Guoco Tower Museum 11 Wallich Street City Hall Clarke Quay 078881 Singapore\n",
      "Guoco :  PlaceName\n",
      "Tower :  PlaceName\n",
      "Museum :  PlaceName\n",
      "11 :  Suburb\n",
      "Wallich :  Suburb\n",
      "Street :  Suburb\n",
      "City :  Suburb\n",
      "Hall :  Suburb\n",
      "Clarke :  Suburb\n",
      "Quay :  UnParsed\n",
      "078881 :  UnParsed\n",
      "Singapore :  UnParsed\n"
     ]
    }
   ],
   "source": [
    "address = 'Guoco Tower Museum 11 Wallich Street City Hall Clarke Quay 078881 Singapore'\n",
    "parseSingleAddress(address, model , vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
