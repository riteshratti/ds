{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##libraries required\n",
    "import math\n",
    "import re\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import numpy\n",
    "import csv\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read DAta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the target values from joineddata\n",
    "pdfJoinedFinal = pd.read_csv('joineddata.csv')\n",
    "YData = pdfJoinedFinal.MAIN_PRODUCT.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383066"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(YData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the features from finalfeatures\n",
    "pdfFeatures = pd.read_csv('finalfeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols= pdfFeatures.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalCols = cols.delete([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WAS_USER_DISPUTED', 'DATE_COMPLAINT_USER', 'COMPANY',\n",
       "       'COMPANY_RESPONSE_TO_USER', 'COMPANY_RESPONSE_TO_PUBLIC',\n",
       "       'WAS_RESPONSE_TIMELY', 'DATE_COMPLAINT_COMPANY', 'MAIN_ISSUE',\n",
       "       'SUB_ISSUE', 'DIFF_DATE',\n",
       "       ...\n",
       "       '990', '991', '992', '993', '994', '995', '996', '997', '998', '999'],\n",
       "      dtype='object', length=1013)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'WAS_USER_DISPUTED', 'DATE_COMPLAINT_USER', 'COMPANY',\n",
       "       'COMPANY_RESPONSE_TO_USER', 'COMPANY_RESPONSE_TO_PUBLIC',\n",
       "       'WAS_RESPONSE_TIMELY', 'DATE_COMPLAINT_COMPANY', 'MAIN_ISSUE',\n",
       "       'SUB_ISSUE',\n",
       "       ...\n",
       "       '990', '991', '992', '993', '994', '995', '996', '997', '998', '999'],\n",
       "      dtype='object', length=1014)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfFeatures.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfFeatures = pdfFeatures[finalCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XData = np.asarray(pdfFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(XData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(XData[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XData = XData[:10000]\n",
    "YData = YData[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(XData, YData, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Vectorizer for whole data set\n",
      "Dictonary complete\n",
      "Saving vectorizer model\n"
     ]
    }
   ],
   "source": [
    "# Use Dict Vectorizer for other text content as the number of uniques are limited . \n",
    "# We have already extracted the tf idf vector for complaint text part\n",
    "print('Creating Vectorizer for whole data set')\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "\n",
    "vectorizer_input=[]\n",
    "\n",
    "for eachRow in X_train:\n",
    "    columnNumber = 0\n",
    "    temp_dic={}\n",
    "    for data in eachRow:\n",
    "        temp_dic[str(columnNumber)] = data\n",
    "        columnNumber = columnNumber +1\n",
    "    vectorizer_input.append(temp_dic)\n",
    "print(\"Dictonary complete\")\n",
    "output_all = vectorizer.fit_transform(vectorizer_input)\n",
    "with open('dictvectorizer.pkl', 'wb') as fid:\n",
    "    pickle.dump(vectorizer, fid)\n",
    "\n",
    "print(\"Saving vectorizer model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MAX ABS SCALER for whole data set\n",
      "Saving MAX ABS SCALER model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Creating MAX ABS SCALER for whole data set')\n",
    "maxabsscaler = MaxAbsScaler()\n",
    "output_maxabs=maxabsscaler.fit_transform(output_all)\n",
    "with open('maxscaler.pkl', 'wb') as fid:\n",
    "    pickle.dump(maxabsscaler, fid)\n",
    "\n",
    "print(\"Saving MAX ABS SCALER model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Vectorizer and scaler\n",
    "output = vectorizer.transform(vectorizer_input)\n",
    "\n",
    "output_transformed = maxabsscaler.transform(output.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiLayer Perceptron\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5,\n",
    "                 hidden_layer_sizes=(100,50), verbose=1, max_iter=10)\n",
    "clf.fit(output_transformed, Y_train) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainedmodel.pkl', 'wb') as fid:\n",
    "    pickle.dump(clf, fid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_prediction(prediction,test_sample): #for batch testing\n",
    "    tp=0\n",
    "    if str(prediction) == str(test_sample):\n",
    "        tp=1\n",
    "    return tp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "startTime = time.time()\n",
    "corr=0\n",
    "total=0\n",
    "predictionvalue = []\n",
    "vectorizerlist=[]\n",
    "for j in range(0, 1):\n",
    "    columnNumber = 0\n",
    "    temp_vectorizer=[]\n",
    "    temp_dic={}\n",
    "    for data in X_test[j]:\n",
    "        temp_dic[str(columnNumber)] = data\n",
    "        columnNumber = columnNumber +1\n",
    "    temp_vectorizer.append(temp_dic)\n",
    "    vectorizerlist.append(temp_vectorizer)\n",
    "    \n",
    "    test_raw_transformed = vectorizer.transform(temp_vectorizer)\n",
    "\n",
    "    print(test_raw_transformed)    \n",
    "    test_raw_scaled = maxabsscaler.transform(test_raw_transformed)\n",
    "\n",
    "    \n",
    "    #print(test_raw_scaled.shape)\n",
    "    #test_raw_scaled_sc = sc.transform(test_raw_transformed.toarray())\n",
    "    #test_raw_scaled_robust = robustsc.transform(test_raw_transformed.toarray())\n",
    "    \n",
    "    prediction = clf.predict(test_raw_scaled.toarray()).tolist()\n",
    "    predictionvalue.append(prediction[0])\n",
    "    #print(\"Prediction : \",prediction[0])\n",
    "    #print(\" Actual: \",test_tag[j])\n",
    "    cor=evaluate_prediction(prediction[0],Y_test[j])\n",
    "    corr=corr+cor\n",
    "    #if cor ==1 and str(test_tag[j]) != 'Normal':\n",
    "        #print(test_raw[j])\n",
    "        #print(test_tag[j])\n",
    "    total=total+1\n",
    "print(str(corr))\n",
    "print(str(total))\n",
    "print('Accuracy is : ' + str(float(corr)/total))\n",
    "endTime= time.time()\n",
    "print(\"Time Taken \"+ str(endTime - startTime) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pandas_ml import ConfusionMatrix\n",
    "import matplotlib.pyplot as plt\n",
    "cm = ConfusionMatrix(y_true=Y_test, y_pred=predictionvalue)\n",
    "cm.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
