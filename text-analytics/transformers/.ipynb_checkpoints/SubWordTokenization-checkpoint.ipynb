{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import boto3\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import time\n",
    "import traceback\n",
    "import warnings\n",
    "import decimal\n",
    "from decimal import Decimal\n",
    "import json\n",
    "\n",
    "import re\n",
    "##libraries required\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import numpy\n",
    "import csv\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tokenizers.trainers import BpeTrainer,WordPieceTrainer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder,WordPiece as WordPieceDecoder\n",
    "from tokenizers.models import BPE,WordPiece\n",
    "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import BertPreTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_id</th>\n",
       "      <th>name</th>\n",
       "      <th>department_name</th>\n",
       "      <th>sub_department_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Mysore Sandal Powder</td>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Feminine Care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Life Alkaline Bottle Water pH 8.5 510ml</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Ben &amp; Jerry  Choc Fud Brownie 458ml [68030002]</td>\n",
       "      <td>Ice Cream &amp; Dessert</td>\n",
       "      <td>Ice Cream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NN016 BoBo Cooked Fishball 250g (26 pcs) I 250 g</td>\n",
       "      <td>Eid al-Fitr</td>\n",
       "      <td>Chilled/Frozen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2 x Tullamore D.E.W. Original, 700ml</td>\n",
       "      <td>Wine, Beers &amp; Spirits</td>\n",
       "      <td>Spirits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country_id                                              name  \\\n",
       "0         4.0                              Mysore Sandal Powder   \n",
       "1         4.0           Life Alkaline Bottle Water pH 8.5 510ml   \n",
       "2         4.0    Ben & Jerry  Choc Fud Brownie 458ml [68030002]   \n",
       "3         4.0  NN016 BoBo Cooked Fishball 250g (26 pcs) I 250 g   \n",
       "4         4.0              2 x Tullamore D.E.W. Original, 700ml   \n",
       "\n",
       "         department_name sub_department_name  \n",
       "0          Personal Care       Feminine Care  \n",
       "1              Beverages               Water  \n",
       "2    Ice Cream & Dessert           Ice Cream  \n",
       "3            Eid al-Fitr      Chilled/Frozen  \n",
       "4  Wine, Beers & Spirits             Spirits  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('item-tags-sg-9may.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN TOKENIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Level BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USed in Roberta Model\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(\"item-tags-sg-9may.csv\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN', '016', 'ĠBo', 'Bo', 'ĠCooked', 'ĠFish', 'ball', 'Ġ250', 'g']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"NN016 BoBo Cooked Fishball 250g\")\n",
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "# Train Encoder\n",
    "spm.SentencePieceTrainer.train('--input=item-tags-sg-9may.csv --model_prefix=m_ind --vocab_size=700')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m_ind.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁T', 'hi', 's', '▁', 'is', '▁', 'a', '▁', 'te', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('This is a test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 204, 5, 3, 174, 3, 9, 3, 160, 5, 13]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_ids('This is a test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# First we create an empty Byte-Pair Encoding model (i.e. not trained model)\n",
    "tokenizer = Tokenizer(WordPiece())\n",
    "\n",
    "# Then we enable lower-casing and unicode-normalization\n",
    "# The Sequence normalizer allows us to combine multiple Normalizer that will be\n",
    "# executed in order.\n",
    "tokenizer.normalizer = Sequence([\n",
    "    NFKC(),\n",
    "    Lowercase()\n",
    "])\n",
    "\n",
    "# Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.\n",
    "tokenizer.pre_tokenizer = BertPreTokenizer()\n",
    "\n",
    "# And finally, let's plug a decoder so we can recover from a tokenized input to the original one\n",
    "tokenizer.decoder = WordPieceDecoder()\n",
    "\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(\"item-tags-sg-9may.csv\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vocab size: 14480\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
    "trainer = WordPieceTrainer(vocab_size=30522, show_progress=True,special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],wordpieces_prefix=\"##\")\n",
    "tokenizer.train(files=paths, trainer=trainer)\n",
    "\n",
    "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nn016', 'bobo', 'cooked', 'fishball', '250g']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"NN016 BoBo Cooked Fishball 250g\")\n",
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ItemTaggingMultiLingualWordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ItemTaggingMultiLingualWordPiece/vocab.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model.save(\"ItemTaggingMultiLingualWordPiece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ItemTaggingMultiLingualWordPiece"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
