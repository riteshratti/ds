{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST NER ON TEST FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3975\n"
     ]
    }
   ],
   "source": [
    "NER_train_Corpus = open ( \"eng.testa\" , 'r')\n",
    "#read the complete corpus\n",
    "corpus= NER_train_Corpus.read()\n",
    "\n",
    "#separate in sentences\n",
    "sentences_in_corpus = sent_tokenize(corpus)\n",
    "\n",
    "#separate the words in each sentence\n",
    "sentences_with_words = []\n",
    "for sent in sentences_in_corpus:\n",
    "    sentences_with_words.append( word_tokenize(sent) )\n",
    "\n",
    "#do some cleaning of corpus\n",
    "#1. remove 'O' , 'O' from start\n",
    "#2. remove sentence with 1 word\n",
    "count = 0\n",
    "length = len(sentences_with_words)\n",
    "print(length)\n",
    "while (count < length):\n",
    "    words = sentences_with_words[count]\n",
    "\n",
    "    if words[0] == '.' : \n",
    "        del sentences_with_words[count]\n",
    "        length = length - 1\n",
    "    elif (words[0] == 'O' and words[1] == 'O' ): \n",
    "        del sentences_with_words[count][0]\n",
    "        del sentences_with_words[count][0]\n",
    "        count = count + 1\n",
    "    else :\n",
    "        count = count + 1\n",
    "\n",
    "NER_train_Corpus.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_first_letter_in_word_capitalized( word ):\n",
    "    if not word.islower() and not word.isupper():    \n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Features \n",
    "\n",
    "Word features/Lexical features: \n",
    "    1) Bag of words - Tokens in the window c = (xi−2, xi−1, xi , xi+1, xi+2)\n",
    "        1) current word \n",
    "        2) previous two words,\n",
    "        3) next two words\n",
    "\n",
    "Syntactic features\n",
    "    1) POS tags for xi−1, xi , xi+1\n",
    "    2) IOB tags for xi−1, xi , xi+1\n",
    "\n",
    "Orthographic features :\n",
    "    1) capitalization pattern in the window c \n",
    "    E.g . Jenny --> Xxxx\n",
    "        . IL-2 --> XX-#\n",
    "    But i have taken sequence of only first alphabet in each word, X if capital , x if small\n",
    "    2) xi word type \n",
    "        1) all-capitalized - > are all letters capitalized\n",
    "        2) is-capitalized -> is the first letter capitalized\n",
    "        5) is it the first word in sentence\n",
    "        6) is it the last word in sentence\n",
    "    3) capitalization pattern in the window c \n",
    "\n",
    "'''\n",
    "\n",
    "def extract_features(sentence,index):\n",
    "        \n",
    "    #tokens in the window c = (xi−2, xi−1, xi , xi+1, xi+2) \n",
    "    token_in_window =[]\n",
    "    token_in_window[0:5] = ['NA','NA','NA','NA','NA'] # 2 words preceeding current word and 2 words after current word\n",
    "    pre_entity_1 = 'NA'\n",
    "    pre_entity_2 = 'NA'\n",
    "    \n",
    "    if(index >= 11) :# this is a word which has two preceeding words to it in the sentence\n",
    "        token_in_window[1] = sentence[index-7]\n",
    "        token_in_window[0] = sentence[index-11]\n",
    "        pre_entity_1 = sentence[index-4]\n",
    "        pre_entity_2 = sentence[index-8]\n",
    "    elif(index==3):# this is first word in the sentence\n",
    "        pre_entity_1 = 'NA'\n",
    "        pre_entity_2 = 'NA'\n",
    "    elif(index==7):#this is second word in sentence\n",
    "        token_in_window[1] = sentence[index-7]\n",
    "        pre_entity_1 = sentence[index-4]\n",
    "        pre_entity_2 = 'NA'\n",
    "\n",
    "    token_in_window[2] = sentence[index-3] #current word\n",
    "    \n",
    "    sent_len = len(sentence) \n",
    "    \n",
    "    if(index+3 >= sent_len) :# this is last word\n",
    "        token_in_window[3] = 'NA'\n",
    "    elif(index+7 >= sent_len):# this is second last word in the sentence\n",
    "        token_in_window[3]= sentence[index+1]\n",
    "    else:\n",
    "        token_in_window[3]= sentence[index+1]\n",
    "        token_in_window[4]= sentence[index+5]\n",
    "    \n",
    "    #get the POS and IOB tags for words in window\n",
    "    \n",
    "    pos_tags=[] \n",
    "    iob_tags=[]\n",
    "    \n",
    "    pos_tags[0:5] = ['NA','NA','NA','NA','NA']\n",
    "    iob_tags[0:5] = ['NA','NA','NA','NA','NA']\n",
    "    \n",
    "    is_first_word_in_sentence = 0\n",
    "    is_last_word_in_sentence = 0\n",
    "    is_all_capitalized = 0 \n",
    "    is_first_letter_capitalized = 0 \n",
    "\n",
    "    if sentence[index-3].isupper():\n",
    "        is_all_capitalized = 1\n",
    "    \n",
    "    is_first_letter_capitalized = is_first_letter_in_word_capitalized(sentence[index-3])\n",
    "    \n",
    "    if (index==3):#first word in sentence\n",
    "        # POS for xi−2 , xi-1 will be NA\n",
    "        pos_tags[2] = sentence[index-2]  #pos for xi\n",
    "        iob_tags[2] =  sentence[index-1] #iob for xi\n",
    "        \n",
    "        pos_tags[3] = sentence[index+2] # POS for xi+1\n",
    "        iob_tags[3] =  sentence[index+3] #iob for xi+1\n",
    "\n",
    "        pos_tags[4] = sentence[index+6]# POS for xi+2\n",
    "        iob_tags[4] =  sentence[index+7] #iob for xi+2\n",
    "        \n",
    "        is_first_word_in_sentence = 1\n",
    "\n",
    "    elif ((index==sent_len-3) or (index==sent_len-2) ):    \n",
    "        # POS for xi+1 , xi+2 will be NA\n",
    "        pos_tags[0] = sentence[index-10]  #pos for xi-2\n",
    "        iob_tags[0] =  sentence[index-9] #iob for xi-2\n",
    "        \n",
    "        pos_tags[1] = sentence[index-6] # POS for xi-1\n",
    "        iob_tags[1] =  sentence[index-5] #iob for xi-1\n",
    "\n",
    "        pos_tags[2] = sentence[index-2]# POS for xi\n",
    "        iob_tags[2] =  sentence[index-1] #iob for xi\n",
    "\n",
    "        is_last_word_in_sentence = 1\n",
    "    \n",
    "    elif(index+7 >= sent_len):# this is second last word in the sentence\n",
    "        pos_tags[0] = sentence[index-10]  #pos for xi-2\n",
    "        iob_tags[0] =  sentence[index-9] #iob for xi-2\n",
    "        \n",
    "        pos_tags[1] = sentence[index-6] # POS for xi-1\n",
    "        iob_tags[1] =  sentence[index-5] #iob for xi-1\n",
    "\n",
    "        pos_tags[2] = sentence[index-2]  #pos for xi\n",
    "        iob_tags[2] =  sentence[index-1] #iob for xi\n",
    "        \n",
    "        pos_tags[3] = sentence[index+2] # POS for xi+1\n",
    "        iob_tags[3] =  sentence[index+3] #iob for xi+1\n",
    "\n",
    "    else:  \n",
    "        pos_tags[0] = sentence[index-10]  #pos for xi-2\n",
    "        iob_tags[0] =  sentence[index-9] #iob for xi-2\n",
    "        \n",
    "        pos_tags[1] = sentence[index-6] # POS for xi-1\n",
    "        iob_tags[1] =  sentence[index-5] #iob for xi-1\n",
    "\n",
    "        pos_tags[2] = sentence[index-2]  #pos for xi\n",
    "        iob_tags[2] =  sentence[index-1] #iob for xi\n",
    "        \n",
    "        pos_tags[3] = sentence[index+2] # POS for xi+1\n",
    "        iob_tags[3] =  sentence[index+3] #iob for xi+1\n",
    "\n",
    "        pos_tags[4] = sentence[index+6]# POS for xi+2\n",
    "        iob_tags[4] =  sentence[index+7] #iob for xi+2\n",
    "          \n",
    "    capitalization_pattern  = str(is_first_letter_in_word_capitalized (token_in_window[0]) )\n",
    "    capitalization_pattern += str(is_first_letter_in_word_capitalized (token_in_window[1]) )\n",
    "    capitalization_pattern += str(is_first_letter_in_word_capitalized (token_in_window[2]) )\n",
    "    capitalization_pattern += str(is_first_letter_in_word_capitalized (token_in_window[3]) )\n",
    "    capitalization_pattern += str(is_first_letter_in_word_capitalized (token_in_window[4]) )\n",
    "    \n",
    "    return {\n",
    "        'pre_word_1' : token_in_window[0],\n",
    "        'pre_word_2' : token_in_window[1],\n",
    "        'cur_word'   : token_in_window[2],\n",
    "        'post_word_1': token_in_window[3],\n",
    "        'post_word_2': token_in_window[4],\n",
    "        \n",
    "        'pre_entity_1':pre_entity_1, # this is extended BIO tag\n",
    "        'pre_entity_2':pre_entity_2,\n",
    "        \n",
    "        'pos_tag_1': pos_tags[0],\n",
    "        'pos_tag_2': pos_tags[1], \n",
    "        'pos_tag_3': pos_tags[2],\n",
    "        'pos_tag_4': pos_tags[3], \n",
    "        'pos_tag_5': pos_tags[4],\n",
    "        \n",
    "        'bio_tag_1': iob_tags[0],\n",
    "        'bio_tag_2': iob_tags[1],\n",
    "        'bio_tag_3': iob_tags[2], \n",
    "        'bio_tag_4': iob_tags[3],\n",
    "        'bio_tag_5': iob_tags[4], \n",
    "        \n",
    "        'fw': is_first_word_in_sentence,\n",
    "        'lw': is_last_word_in_sentence,\n",
    "        'acap': is_all_capitalized,\n",
    "        'fcap': is_first_letter_capitalized,\n",
    "        'cap_ptrn':capitalization_pattern\n",
    "       }       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2091\n"
     ]
    }
   ],
   "source": [
    "featuresets =[]\n",
    "'''\n",
    "The CONLL format is \n",
    "LEICESTERSHIRE NNP I-NP I-ORG\n",
    "i.e \n",
    "<word> <POStag> <IOB tag> <IOB tag for Organizayion>\n",
    "\n",
    "Hence i am parsing the sentence in similar way to extract features\n",
    "'''\n",
    "sentenceFeatures = []\n",
    "tags = []\n",
    "for sentence in sentences_with_words:\n",
    "    featuresets = []\n",
    "    sent_len = len(sentence) \n",
    "    num_word = (sent_len-1)/4\n",
    "\n",
    "    if(num_word < 3): #in some case the sentence is of just 1 or 2 words. I am avoiding those sentences for now\n",
    "        continue\n",
    "    count = 0\n",
    "    while (count < num_word-1):\n",
    "        IOB_index = (count * 4 )+ 3\n",
    "        if ( sentence[IOB_index] == 'I-ORG'):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'ORG') )\n",
    "        elif ( sentence[IOB_index] == 'I-LOC'):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'LOC') )\n",
    "        elif ( sentence[IOB_index] == 'I-PER'):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'PER') )\n",
    "        elif ( sentence[IOB_index] == 'I-MISC'):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'MISC') )\n",
    "       \n",
    "        count = count + 1\n",
    "    sentenceFeatures.append(featuresets)\n",
    "    \n",
    "print(len(sentenceFeatures))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_test=[[]]\n",
    "Y_test = [[]]\n",
    "\n",
    "lengthtest = len(sentenceFeatures)\n",
    "print(lengthtest)\n",
    "\n",
    "for sent in sentenceFeatures[0:lengthtest]:\n",
    "    #print sent\n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    for word in sent:\n",
    "        temp1.append(word[0])\n",
    "        temp2.append(word[1])\n",
    "    X_test.append(temp1)\n",
    "    Y_test.append(temp2)\n",
    "    \n",
    "    temp1 = []\n",
    "    temp2 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x7fad103a4070>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('conll2002-esp.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PER']\n",
      "['ORG']\n"
     ]
    }
   ],
   "source": [
    "print(tagger.tag(X_test[1]))\n",
    "print(Y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2092\n",
      "6920\n",
      "     |         M           |\n",
      "     |    L    I    O    P |\n",
      "     |    O    S    R    E |\n",
      "     |    C    C    G    R |\n",
      "-----+---------------------+\n",
      " LOC |<1350>  58  114  118 |\n",
      "MISC |   61 <896>  58   59 |\n",
      " ORG |   79   33<1573> 166 |\n",
      " PER |   61   12   31<2251>|\n",
      "-----+---------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "Y_pred=[]\n",
    "for row in X_test:\n",
    "    Y_pred.append(tagger.tag(row))\n",
    "print(len(Y_pred))\n",
    "new_y_Pred=[]\n",
    "new_y_Test=[]\n",
    "for row in Y_test:\n",
    "    new_y_Test=new_y_Test+row\n",
    "\n",
    "for row in Y_pred:\n",
    "    new_y_Pred=new_y_Pred+row\n",
    "print(len(new_y_Test))\n",
    "#Y_pred = tagger.tag(X_test[1])\n",
    "cm = ConfusionMatrix(new_y_Test, new_y_Pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "true_positives = Counter()\n",
    "false_negatives = Counter()\n",
    "false_positives = Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 6070 Counter({'PER': 2251, 'ORG': 1573, 'LOC': 1350, 'MISC': 896})\n",
      "FN: 850 Counter({'LOC': 290, 'ORG': 278, 'MISC': 178, 'PER': 104})\n",
      "FP: 850 Counter({'PER': 343, 'ORG': 203, 'LOC': 201, 'MISC': 103})\n",
      "Accuracy 78.12097812097812\n",
      "Fscore for each label\n",
      "LOC 0.8461297398934503\n",
      "MISC 0.864447660395562\n",
      "ORG 0.8673835125448028\n",
      "PER 0.9096787229743382\n"
     ]
    }
   ],
   "source": [
    "labels = set('LOC MISC ORG PER'.split())\n",
    "for i in labels:\n",
    "    for j in labels:\n",
    "        if i == j:\n",
    "            true_positives[i] += cm[i,j]\n",
    "        else:\n",
    "            false_negatives[i] += cm[i,j]\n",
    "            false_positives[j] += cm[i,j]\n",
    "\n",
    "print(\"TP:\", sum(true_positives.values()), true_positives)\n",
    "print(\"FN:\", sum(false_negatives.values()), false_negatives)\n",
    "print(\"FP:\", sum(false_positives.values()), false_positives)\n",
    "\n",
    "print(\"Accuracy\", float(sum(true_positives.values()))*100/(sum(true_positives.values()) + sum(false_negatives.values()) + sum(false_positives.values())))\n",
    "\n",
    "print(\"Fscore for each label\")\n",
    "for i in sorted(labels):\n",
    "    if true_positives[i] == 0:\n",
    "        fscore = 0\n",
    "    else:\n",
    "        precision = true_positives[i] / float(true_positives[i]+false_positives[i])\n",
    "        recall = true_positives[i] / float(true_positives[i]+false_negatives[i])\n",
    "        fscore = 2 * (precision * recall) / float(precision + recall)\n",
    "    print(i, fscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
