{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample code for NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere in this notebook, I am doing the experiment using CONLL corpus \\nI have to clean the data a bit\\n\\nand have then extracted features for organization \\nfor all the other words in the coprus , i put them in OTHER category and then doing the prediction using SVM and logistic \\nregression\\n\\nHere the approach to follow is\\nmake 4 classes for person, location, organization, misc\\nmake 5th class for others ( like verb , CD )\\nand then do classification for all these entities in one go\\n\\n\\nbut the question is - will the feature for organization be similar to name or location.\\nWell most of them are .. so for non relevant features we can put NA\\n\\nelse look for some advance approach, like adding random forest , e.g\\nhttps://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words \\n'"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here in this notebook, I am doing the experiment using CONLL corpus \n",
    "I have to clean the data a bit\n",
    "\n",
    "and have then extracted features for organization \n",
    "for all the other words in the coprus , i put them in OTHER category and then doing the prediction using SVM and logistic \n",
    "regression\n",
    "\n",
    "Here the approach to follow is\n",
    "make 4 classes for person, location, organization, misc\n",
    "make 5th class for others ( like verb , CD )\n",
    "and then do classification for all these entities in one go\n",
    "\n",
    "\n",
    "but the question is - will the feature for organization be similar to name or location.\n",
    "Well most of them are .. so for non relevant features we can put NA\n",
    "\n",
    "else look for some advance approach, like adding random forest , e.g\n",
    "https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3975\n"
     ]
    }
   ],
   "source": [
    "NER_train_Corpus = open ( \"C:\\\\share\\\\Co-nll\\\\eng.testa\" , 'r')\n",
    "#read the complete corpus\n",
    "corpus= NER_train_Corpus.read()\n",
    "\n",
    "#separate in sentences\n",
    "sentences_in_corpus = sent_tokenize(corpus)\n",
    "\n",
    "#separate the words in each sentence\n",
    "sentences_with_words = []\n",
    "for sent in sentences_in_corpus:\n",
    "    sentences_with_words.append( word_tokenize(sent) )\n",
    "\n",
    "#do some cleaning of corpus\n",
    "#1. remove 'O' , 'O' from start\n",
    "#2. remove sentence with 1 word\n",
    "count = 0\n",
    "length = len(sentences_with_words)\n",
    "print length\n",
    "while (count < length):\n",
    "    words = sentences_with_words[count]\n",
    "\n",
    "    if words[0] == '.' : \n",
    "        del sentences_with_words[count]\n",
    "        length = length - 1\n",
    "    elif (words[0] == 'O' and words[1] == 'O' ): \n",
    "        del sentences_with_words[count][0]\n",
    "        del sentences_with_words[count][0]\n",
    "        count = count + 1\n",
    "    else :\n",
    "        count = count + 1\n",
    "\n",
    "NER_train_Corpus.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_first_letter_in_word_capitalized( word ):\n",
    "    if not word.islower() and not word.isupper():    \n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Features \n",
    "\n",
    "Word features/Lexical features: \n",
    "    1) Bag of words - Tokens in the window c = (xi−2, xi−1, xi , xi+1, xi+2)\n",
    "        1) current word \n",
    "        2) previous two words,\n",
    "        3) next two words\n",
    "\n",
    "Syntactic features\n",
    "    1) POS tags for xi−1, xi , xi+1\n",
    "    2) IOB tags for xi−1, xi , xi+1\n",
    "\n",
    "Orthographic features :\n",
    "    1) capitalization pattern in the window c \n",
    "    E.g . Jenny --> Xxxx\n",
    "        . IL-2 --> XX-#\n",
    "    But i have taken sequence of only first alphabet in each word, X if capital , x if small\n",
    "    2) xi word type \n",
    "        1) all-capitalized - > are all letters capitalized\n",
    "        2) is-capitalized -> is the first letter capitalized\n",
    "        5) is it the first word in sentence\n",
    "        6) is it the last word in sentence\n",
    "    3) capitalization pattern in the window c \n",
    "\n",
    "\n",
    "### Not used features\n",
    "Word features :\n",
    "    1) Trigger words - (e.g. Inc., Co., Mr.).\n",
    "\n",
    "Linguistic features :\n",
    "    1. dictionaries\n",
    "    2. Word-net\n",
    "    3. Custom lists\n",
    "    4. Gazetteer based recognition\n",
    "\n",
    "Orthographic features :\n",
    "    1. prefixes and suffixes of xi \n",
    "    2. conjunction of c and yi−1.\n",
    "    3. all-digits\n",
    "    4. alphanumeric\n",
    "    5. Regular expressions\n",
    "    \n",
    "other features that can be used \n",
    "\n",
    ". Topic Feature \n",
    ". Word2Vec feature\n",
    "\n",
    "\n",
    "\n",
    "If you can look up the whole document , then Context aggregation, Extended prediction history can be used\n",
    "'''\n",
    "\n",
    "def extract_features(sentence,index):\n",
    "        \n",
    "    #tokens in the window c = (xi−2, xi−1, xi , xi+1, xi+2) \n",
    "    token_in_window =[]\n",
    "    token_in_window[0:5] = ['NA','NA','NA','NA','NA'] # 2 words preceeding current word and 2 words after current word\n",
    "    pre_entity_1 = 'NA'\n",
    "    pre_entity_2 = 'NA'\n",
    "    \n",
    "    if(index >= 11) :# this is a word which has two preceeding words to it in the sentence\n",
    "        token_in_window[1] = sentence[index-7]\n",
    "        token_in_window[0] = sentence[index-11]\n",
    "        pre_entity_1 = sentence[index-4]\n",
    "        pre_entity_2 = sentence[index-8]\n",
    "    elif(index==3):# this is first word in the sentence\n",
    "        pre_entity_1 = 'NA'\n",
    "        pre_entity_2 = 'NA'\n",
    "    elif(index==7):#this is second word in sentence\n",
    "        token_in_window[1] = sentence[index-7]\n",
    "        pre_entity_1 = sentence[index-4]\n",
    "        pre_entity_2 = 'NA'\n",
    "\n",
    "    token_in_window[2] = sentence[index-3] #current word\n",
    "    \n",
    "    sent_len = len(sentence) \n",
    "    \n",
    "    if(index+3 >= sent_len) :# this is last word\n",
    "        token_in_window[3] = 'NA'\n",
    "    elif(index+7 >= sent_len):# this is second last word in the sentence\n",
    "        token_in_window[3]= sentence[index+1]\n",
    "    else:\n",
    "        token_in_window[3]= sentence[index+1]\n",
    "        token_in_window[4]= sentence[index+5]\n",
    "    \n",
    "    #get the POS and IOB tags for words in window\n",
    "    \n",
    "    pos_tags=[] \n",
    "    iob_tags=[]\n",
    "    \n",
    "    pos_tags[0:5] = ['NA','NA','NA','NA','NA']\n",
    "    iob_tags[0:5] = ['NA','NA','NA','NA','NA']\n",
    "    \n",
    "    is_first_word_in_sentence = 0\n",
    "    is_last_word_in_sentence = 0\n",
    "    is_all_capitalized = 0 \n",
    "    is_first_letter_capitalized = 0 \n",
    "\n",
    "    if sentence[index-3].isupper():\n",
    "        is_all_capitalized = 1\n",
    "    \n",
    "    is_first_letter_capitalized = is_first_letter_in_word_capitalized(sentence[index-3])\n",
    "    \n",
    "    if (index==3):#first word in sentence\n",
    "        # POS for xi−2 , xi-1 will be NA\n",
    "        pos_tags[2] = sentence[index-2]  #pos for xi\n",
    "        iob_tags[2] =  sentence[index-1] #iob for xi\n",
    "        \n",
    "        pos_tags[3] = sentence[index+2] # POS for xi+1\n",
    "        iob_tags[3] =  sentence[index+3] #iob for xi+1\n",
    "\n",
    "        pos_tags[4] = sentence[index+6]# POS for xi+2\n",
    "        iob_tags[4] =  sentence[index+7] #iob for xi+2\n",
    "        \n",
    "        is_first_word_in_sentence = 1\n",
    "\n",
    "    elif ((index==sent_len-3) or (index==sent_len-2) ):    \n",
    "        # POS for xi+1 , xi+2 will be NA\n",
    "        pos_tags[0] = sentence[index-10]  #pos for xi-2\n",
    "        iob_tags[0] =  sentence[index-9] #iob for xi-2\n",
    "        \n",
    "        pos_tags[1] = sentence[index-6] # POS for xi-1\n",
    "        iob_tags[1] =  sentence[index-5] #iob for xi-1\n",
    "\n",
    "        pos_tags[2] = sentence[index-2]# POS for xi\n",
    "        iob_tags[2] =  sentence[index-1] #iob for xi\n",
    "\n",
    "        is_last_word_in_sentence = 1\n",
    "    \n",
    "    elif(index+7 >= sent_len):# this is second last word in the sentence\n",
    "        pos_tags[0] = sentence[index-10]  #pos for xi-2\n",
    "        iob_tags[0] =  sentence[index-9] #iob for xi-2\n",
    "        \n",
    "        pos_tags[1] = sentence[index-6] # POS for xi-1\n",
    "        iob_tags[1] =  sentence[index-5] #iob for xi-1\n",
    "\n",
    "        pos_tags[2] = sentence[index-2]  #pos for xi\n",
    "        iob_tags[2] =  sentence[index-1] #iob for xi\n",
    "        \n",
    "        pos_tags[3] = sentence[index+2] # POS for xi+1\n",
    "        iob_tags[3] =  sentence[index+3] #iob for xi+1\n",
    "\n",
    "    else:  \n",
    "        pos_tags[0] = sentence[index-10]  #pos for xi-2\n",
    "        iob_tags[0] =  sentence[index-9] #iob for xi-2\n",
    "        \n",
    "        pos_tags[1] = sentence[index-6] # POS for xi-1\n",
    "        iob_tags[1] =  sentence[index-5] #iob for xi-1\n",
    "\n",
    "        pos_tags[2] = sentence[index-2]  #pos for xi\n",
    "        iob_tags[2] =  sentence[index-1] #iob for xi\n",
    "        \n",
    "        pos_tags[3] = sentence[index+2] # POS for xi+1\n",
    "        iob_tags[3] =  sentence[index+3] #iob for xi+1\n",
    "\n",
    "        pos_tags[4] = sentence[index+6]# POS for xi+2\n",
    "        iob_tags[4] =  sentence[index+7] #iob for xi+2\n",
    "          \n",
    "    capitalization_pattern  = str(is_first_letter_in_word_capitalized (token_in_window[0]) )\n",
    "    capitalization_pattern += str(is_first_letter_in_word_capitalized (token_in_window[1]) )\n",
    "    capitalization_pattern += str(is_first_letter_in_word_capitalized (token_in_window[2]) )\n",
    "    capitalization_pattern += str(is_first_letter_in_word_capitalized (token_in_window[3]) )\n",
    "    capitalization_pattern += str(is_first_letter_in_word_capitalized (token_in_window[4]) )\n",
    "    \n",
    "    return {\n",
    "        #'pre_word_1' : token_in_window[0],\n",
    "        'pre_word_2' : token_in_window[1],\n",
    "        'cur_word'   : token_in_window[2],\n",
    "        'post_word_1': token_in_window[3],\n",
    "        #'post_word_2': token_in_window[4],\n",
    "        \n",
    "        'pre_entity_1':pre_entity_1, # this is extended BIO tag\n",
    "        'pre_entity_2':pre_entity_2,\n",
    "        \n",
    "        #'pos_tag_1': pos_tags[0],\n",
    "        'pos_tag_2': pos_tags[1], \n",
    "        'pos_tag_3': pos_tags[2],\n",
    "        'pos_tag_4': pos_tags[3], \n",
    "        #'pos_tag_5': pos_tags[4],\n",
    "        \n",
    "        #'bio_tag_1': iob_tags[0],\n",
    "        'bio_tag_2': iob_tags[1],\n",
    "        'bio_tag_3': iob_tags[2], \n",
    "        'bio_tag_4': iob_tags[3],\n",
    "        #'bio_tag_5': iob_tags[4], \n",
    "        \n",
    "        'fw': is_first_word_in_sentence,\n",
    "        'lw': is_last_word_in_sentence,\n",
    "        'acap': is_all_capitalized,\n",
    "        'fcap': is_first_letter_capitalized,\n",
    "        #'cap_ptrn':capitalization_pattern\n",
    "       }       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We need to take following out from the input sentence\n",
    "    VB, VBD,VBG,VBZ, CC, CD, IN, NN, DT, NNS, PRP$,RB, JJ, RP, TO,PRP\n",
    "even before supplying to classifier. \n",
    "\n",
    "This is needed becuase else we will end up in creating imbalanced data set with 100 time more entries for OTHER categories\n",
    "\n",
    "Advance concept\n",
    "===============\n",
    "    It is mostly NNP for all entities but\n",
    "\n",
    "    JJ and NNPS -  come in I-PER \n",
    "    JJ , 's , DT followed by JJ ( The Oval) - come in I-LOC \n",
    "\n",
    "    For I-ORG , Syntactic features seems important\n",
    "        i.\tDependency path\n",
    "        ii.\tIOB chain\n",
    "    Like The Test and County Criket Board, \n",
    "'''\n",
    "def include_for_others(sentence,IOB_index):\n",
    "    #took VBD I-VP O\n",
    "    #Friday NNP I-NP O\n",
    "    if sentence[IOB_index-2] == 'NNP' or sentence[IOB_index-2] == 'IN':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2091\n"
     ]
    }
   ],
   "source": [
    "featuresets =[]\n",
    "'''\n",
    "The CONLL format is \n",
    "LEICESTERSHIRE NNP I-NP I-ORG\n",
    "i.e \n",
    "<word> <POStag> <IOB tag> <IOB tag for Organizayion>\n",
    "\n",
    "Hence i am parsing the sentence in similar way to extract features\n",
    "'''\n",
    "sentenceFeatures = []\n",
    "tags = []\n",
    "for sentence in sentences_with_words:\n",
    "    featuresets = []\n",
    "    sent_len = len(sentence) \n",
    "    num_word = (sent_len-1)/4\n",
    "\n",
    "    if(num_word < 3): #in some case the sentence is of just 1 or 2 words. I am avoiding those sentences for now\n",
    "        continue\n",
    "    count = 0\n",
    "    while (count < num_word):\n",
    "        IOB_index = (count * 4 )+ 3\n",
    "        if ( sentence[IOB_index] == 'I-ORG'):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'ORG') )\n",
    "        elif ( sentence[IOB_index] == 'I-LOC'):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'LOC') )\n",
    "        elif ( sentence[IOB_index] == 'I-PER'):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'PER') )\n",
    "        elif ( sentence[IOB_index] == 'I-MISC'):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'MISC') )\n",
    "        elif include_for_others(sentence,IOB_index):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'OTHER') )\n",
    " \n",
    "        count = count + 1\n",
    "    sentenceFeatures.append(featuresets)\n",
    "    \n",
    "print len(sentenceFeatures)\n",
    "#print featuresets\n",
    "#Sample feature set should be list of tuples which should look like \n",
    "#[({'ed': 4, 'rel_type': 'PL', 'ps2': 0, 'ps1': 0, 'e1': 'Person', 'e2': 'Location'}, 'positive'), \n",
    "#({'ed': 1, 'rel_type': 'CL', 'ps2': 0, 'ps1': 0, 'e1': 'Company', 'e2': 'Location'}, 'positive'), \n",
    "#({'ed': 2, 'rel_type': 'PL', 'ps2': 0, 'ps1': 1, 'e1': 'Person', 'e2': 'Location'}, 'negative')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "2091\n",
      "1045\n",
      "Wall time: 29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "number_of_sample_to_take_crf = len(sentenceFeatures)\n",
    "partition = 2\n",
    "\n",
    "print len(sentenceFeatures[0])\n",
    "crftrain_set, crftest_set = sentenceFeatures[0:number_of_sample_to_take_crf/partition], sentenceFeatures[number_of_sample_to_take_crf/partition + 1:]\n",
    "print len(sentenceFeatures)\n",
    "print len(crftrain_set)\n",
    "\n",
    "\n",
    "X_train=[[]]\n",
    "Y_train = [[]]\n",
    "\n",
    "length = len(crftrain_set)\n",
    "for sent in crftrain_set[0:length]:\n",
    "    #print sent\n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    for word in sent:\n",
    "        temp1.append(word[0])\n",
    "        temp2.append(word[1])\n",
    "    X_train.append(temp1)\n",
    "    Y_train.append(temp2)\n",
    "    \n",
    "    \n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045\n"
     ]
    }
   ],
   "source": [
    "X_test=[[]]\n",
    "Y_test = [[]]\n",
    "\n",
    "lengthtest = len(crftest_set)\n",
    "print lengthtest\n",
    "\n",
    "for sent in crftest_set[0:lengthtest]:\n",
    "    #print sent\n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    for word in sent:\n",
    "        temp1.append(word[0])\n",
    "        temp2.append(word[1])\n",
    "    X_test.append(temp1)\n",
    "    Y_test.append(temp2)\n",
    "    \n",
    "    temp1 = []\n",
    "    temp2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PER', 'PER']\n"
     ]
    }
   ],
   "source": [
    "print Y_test[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(X_train, Y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 585 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('conll2002-esp.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 {'loss': 1809.927857, 'error_norm': 0.833546, 'linesearch_trials': 1, 'active_features': 7361, 'num': 117, 'time': 0.004, 'scores': {}, 'linesearch_step': 1.0, 'feature_norm': 23.80522}\n"
     ]
    }
   ],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "trainer.logparser.last_iteration\n",
    "print len(trainer.logparser.iterations), trainer.logparser.iterations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0xbf72278>"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('conll2002-esp.crfsuite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOC', 'MISC', 'OTHER', 'OTHER']\n",
      "['LOC', 'MISC', 'OTHER', 'OTHER']\n"
     ]
    }
   ],
   "source": [
    "print tagger.tag(X_test[1])\n",
    "print Y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1046\n",
      "6847\n",
      "      |                   O      |\n",
      "      |         M         T      |\n",
      "      |    L    I    O    H    P |\n",
      "      |    O    S    R    E    E |\n",
      "      |    C    C    G    R    R |\n",
      "------+--------------------------+\n",
      "  LOC | <742>  32   68   58   87 |\n",
      " MISC |   39 <496>  59   32   30 |\n",
      "  ORG |   82   23 <890>  29   50 |\n",
      "OTHER |  121   23   46<2429>  52 |\n",
      "  PER |   48   10   18   17<1366>|\n",
      "------+--------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "Y_pred=[]\n",
    "for row in X_test:\n",
    "    Y_pred.append(tagger.tag(row))\n",
    "print len(Y_pred)\n",
    "new_y_Pred=[]\n",
    "new_y_Test=[]\n",
    "for row in Y_test:\n",
    "    new_y_Test=new_y_Test+row\n",
    "\n",
    "for row in Y_pred:\n",
    "    new_y_Pred=new_y_Pred+row\n",
    "print len(new_y_Test)\n",
    "#Y_pred = tagger.tag(X_test[1])\n",
    "cm = ConfusionMatrix(new_y_Test, new_y_Pred)\n",
    "print cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "true_positives = Counter()\n",
    "false_negatives = Counter()\n",
    "false_positives = Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 5923 Counter({'OTHER': 2429, 'PER': 1366, 'ORG': 890, 'LOC': 742, 'MISC': 496})\n",
      "FN: 924 Counter({'LOC': 245, 'OTHER': 242, 'ORG': 184, 'MISC': 160, 'PER': 93})\n",
      "FP: 924 Counter({'LOC': 290, 'PER': 219, 'ORG': 191, 'OTHER': 136, 'MISC': 88})\n",
      "Accuracy 76.2192767984\n",
      "\n",
      "Fscore for each label\n",
      "LOC 0.735017335315\n",
      "MISC 0.8\n",
      "ORG 0.825986078886\n",
      "OTHER 0.927807486631\n",
      "PER 0.897503285151\n"
     ]
    }
   ],
   "source": [
    "\n",
    "labels = set('LOC MISC ORG OTHER PER'.split())\n",
    "for i in labels:\n",
    "    for j in labels:\n",
    "        if i == j:\n",
    "            true_positives[i] += cm[i,j]\n",
    "        else:\n",
    "            false_negatives[i] += cm[i,j]\n",
    "            false_positives[j] += cm[i,j]\n",
    "\n",
    "print \"TP:\", sum(true_positives.values()), true_positives\n",
    "print \"FN:\", sum(false_negatives.values()), false_negatives\n",
    "print \"FP:\", sum(false_positives.values()), false_positives\n",
    "\n",
    "print \"Accuracy\", float(sum(true_positives.values()))*100/(sum(true_positives.values()) + sum(false_negatives.values()) + sum(false_positives.values()))\n",
    "print \n",
    "\n",
    "print \"Fscore for each label\"\n",
    "for i in sorted(labels):\n",
    "    if true_positives[i] == 0:\n",
    "        fscore = 0\n",
    "    else:\n",
    "        precision = true_positives[i] / float(true_positives[i]+false_positives[i])\n",
    "        recall = true_positives[i] / float(true_positives[i]+false_negatives[i])\n",
    "        fscore = 2 * (precision * recall) / float(precision + recall)\n",
    "    print i, fscore\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
