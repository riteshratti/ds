{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample code for NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere in this notebook, I am doing the experiment using CONLL corpus \\nI have to clean the data a bit\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here in this notebook, I am doing the experiment using CONLL corpus \n",
    "I have to clean the data a bit\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15611\n"
     ]
    }
   ],
   "source": [
    "NER_train_Corpus = open ( \"D:\\\\pb_anshul\\\\temp\\\\temp_prj\\\\nlp_prj\\\\NER\\\\Co-nll-data\\\\eng.train\" , 'r')\n",
    "#read the complete corpus\n",
    "corpus= NER_train_Corpus.read()\n",
    "\n",
    "#separate in sentences\n",
    "sentences_in_corpus = sent_tokenize(corpus)\n",
    "\n",
    "#separate the words in each sentence\n",
    "sentences_with_words = []\n",
    "for sent in sentences_in_corpus:\n",
    "    sentences_with_words.append( word_tokenize(sent) )\n",
    "\n",
    "#do some cleaning of corpus\n",
    "#1. remove 'O' , 'O' from start\n",
    "#2. remove sentence with 1 word\n",
    "count = 0\n",
    "length = len(sentences_with_words)\n",
    "print length\n",
    "while (count < length):\n",
    "    words = sentences_with_words[count]\n",
    "\n",
    "    if words[0] == '.' : \n",
    "        del sentences_with_words[count]\n",
    "        length = length - 1\n",
    "    elif (words[0] == 'O' and words[1] == 'O' ): \n",
    "        del sentences_with_words[count][0]\n",
    "        del sentences_with_words[count][0]\n",
    "        count = count + 1\n",
    "    else :\n",
    "        count = count + 1\n",
    "\n",
    "NER_train_Corpus.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_first_letter_in_word_capitalized( word ):\n",
    "    if not word.islower() and not word.isupper():    \n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Features \n",
    "\n",
    "Word features/Lexical features: \n",
    "    1) Bag of words - Tokens in the window c = (xi−2, xi−1, xi , xi+1, xi+2)\n",
    "        1) current word \n",
    "        2) previous two words,\n",
    "        3) next two words\n",
    "\n",
    "Syntactic features\n",
    "    1) POS tags for xi−1, xi , xi+1\n",
    "    2) IOB tags for xi−1, xi , xi+1\n",
    "\n",
    "Orthographic features :\n",
    "    1) capitalization pattern in the window c \n",
    "    E.g . Jenny --> Xxxx\n",
    "        . IL-2 --> XX-#\n",
    "    But i have taken sequence of only first alphabet in each word, X if capital , x if small\n",
    "    2) xi word type \n",
    "        1) all-capitalized - > are all letters capitalized\n",
    "        2) is-capitalized -> is the first letter capitalized\n",
    "        5) is it the first word in sentence\n",
    "        6) is it the last word in sentence\n",
    "    3) capitalization pattern in the window c \n",
    "\n",
    "\n",
    "### Not used features\n",
    "Word features :\n",
    "    1) Trigger words - (e.g. Inc., Co., Mr.).\n",
    "\n",
    "Linguistic features :\n",
    "    1. dictionaries\n",
    "    2. Word-net\n",
    "    3. Custom lists\n",
    "    4. Gazetteer based recognition\n",
    "\n",
    "Orthographic features :\n",
    "    1. prefixes and suffixes of xi \n",
    "    2. conjunction of c and yi−1.\n",
    "    3. all-digits\n",
    "    4. alphanumeric\n",
    "    5. Regular expressions\n",
    "    \n",
    "other features that can be used \n",
    "\n",
    ". Topic Feature \n",
    ". Word2Vec feature\n",
    "\n",
    "\n",
    "\n",
    "If you can look up the whole document , then Context aggregation, Extended prediction history can be used\n",
    "'''\n",
    "\n",
    "def extract_features(sentence,index):\n",
    "        \n",
    "    #tokens in the window c = (xi−2, xi−1, xi , xi+1, xi+2) \n",
    "    token_in_window =[]\n",
    "    token_in_window[0:5] = ['NA','NA','NA','NA','NA'] # 2 words preceeding current word and 2 words after current word\n",
    "    pre_entity_1 = 'NA'\n",
    "    pre_entity_2 = 'NA'\n",
    "    \n",
    "    if(index >= 11) :# this is a word which has two preceeding words to it in the sentence\n",
    "        token_in_window[1] = sentence[index-7]\n",
    "        token_in_window[0] = sentence[index-11]\n",
    "        pre_entity_1 = sentence[index-4]\n",
    "        pre_entity_2 = sentence[index-8]\n",
    "    elif(index==3):# this is first word in the sentence\n",
    "        pre_entity_1 = 'NA'\n",
    "        pre_entity_2 = 'NA'\n",
    "    elif(index==7):#this is second word in sentence\n",
    "        token_in_window[1] = sentence[index-7]\n",
    "        pre_entity_1 = sentence[index-4]\n",
    "        pre_entity_2 = 'NA'\n",
    "\n",
    "    token_in_window[2] = sentence[index-3] #current word\n",
    "    \n",
    "    sent_len = len(sentence) \n",
    "    \n",
    "    if(index+3 >= sent_len) :# this is last word\n",
    "        token_in_window[3] = 'NA'\n",
    "    elif(index+7 >= sent_len):# this is second last word in the sentence\n",
    "        token_in_window[3]= sentence[index+1]\n",
    "    else:\n",
    "        token_in_window[3]= sentence[index+1]\n",
    "        token_in_window[4]= sentence[index+5]\n",
    "    \n",
    "    #get the POS and IOB tags for words in window\n",
    "    \n",
    "    pos_tags=[] \n",
    "    iob_tags=[]\n",
    "    \n",
    "    pos_tags[0:5] = ['NA','NA','NA','NA','NA']\n",
    "    iob_tags[0:5] = ['NA','NA','NA','NA','NA']\n",
    "    \n",
    "    is_first_word_in_sentence = 0\n",
    "    is_last_word_in_sentence = 0\n",
    "    is_all_capitalized = 0 \n",
    "    is_first_letter_capitalized = 0 \n",
    "\n",
    "    if sentence[index-3].isupper():\n",
    "        is_all_capitalized = 1\n",
    "    \n",
    "    is_first_letter_capitalized = is_first_letter_in_word_capitalized(sentence[index-3])\n",
    "    \n",
    "    if (index==3):#first word in sentence\n",
    "        # POS for xi−2 , xi-1 will be NA\n",
    "        pos_tags[2] = sentence[index-2]  #pos for xi\n",
    "        iob_tags[2] =  sentence[index-1] #iob for xi\n",
    "        \n",
    "        pos_tags[3] = sentence[index+2] # POS for xi+1\n",
    "        iob_tags[3] =  sentence[index+3] #iob for xi+1\n",
    "\n",
    "        pos_tags[4] = sentence[index+6]# POS for xi+2\n",
    "        iob_tags[4] =  sentence[index+7] #iob for xi+2\n",
    "        \n",
    "        is_first_word_in_sentence = 1\n",
    "\n",
    "    elif ((index==sent_len-3) or (index==sent_len-2) ):    \n",
    "        # POS for xi+1 , xi+2 will be NA\n",
    "        pos_tags[0] = sentence[index-10]  #pos for xi-2\n",
    "        iob_tags[0] =  sentence[index-9] #iob for xi-2\n",
    "        \n",
    "        pos_tags[1] = sentence[index-6] # POS for xi-1\n",
    "        iob_tags[1] =  sentence[index-5] #iob for xi-1\n",
    "\n",
    "        pos_tags[2] = sentence[index-2]# POS for xi\n",
    "        iob_tags[2] =  sentence[index-1] #iob for xi\n",
    "\n",
    "        is_last_word_in_sentence = 1\n",
    "    \n",
    "    elif(index+7 >= sent_len):# this is second last word in the sentence\n",
    "        pos_tags[0] = sentence[index-10]  #pos for xi-2\n",
    "        iob_tags[0] =  sentence[index-9] #iob for xi-2\n",
    "        \n",
    "        pos_tags[1] = sentence[index-6] # POS for xi-1\n",
    "        iob_tags[1] =  sentence[index-5] #iob for xi-1\n",
    "\n",
    "        pos_tags[2] = sentence[index-2]  #pos for xi\n",
    "        iob_tags[2] =  sentence[index-1] #iob for xi\n",
    "        \n",
    "        pos_tags[3] = sentence[index+2] # POS for xi+1\n",
    "        iob_tags[3] =  sentence[index+3] #iob for xi+1\n",
    "\n",
    "    else:  \n",
    "        pos_tags[0] = sentence[index-10]  #pos for xi-2\n",
    "        iob_tags[0] =  sentence[index-9] #iob for xi-2\n",
    "        \n",
    "        pos_tags[1] = sentence[index-6] # POS for xi-1\n",
    "        iob_tags[1] =  sentence[index-5] #iob for xi-1\n",
    "\n",
    "        pos_tags[2] = sentence[index-2]  #pos for xi\n",
    "        iob_tags[2] =  sentence[index-1] #iob for xi\n",
    "        \n",
    "        pos_tags[3] = sentence[index+2] # POS for xi+1\n",
    "        iob_tags[3] =  sentence[index+3] #iob for xi+1\n",
    "\n",
    "        pos_tags[4] = sentence[index+6]# POS for xi+2\n",
    "        iob_tags[4] =  sentence[index+7] #iob for xi+2\n",
    "          \n",
    "    capitalization_pattern  = str(is_first_letter_in_word_capitalized (token_in_window[0]) )\n",
    "    capitalization_pattern += str(is_first_letter_in_word_capitalized (token_in_window[1]) )\n",
    "    capitalization_pattern += str(is_first_letter_in_word_capitalized (token_in_window[2]) )\n",
    "    capitalization_pattern += str(is_first_letter_in_word_capitalized (token_in_window[3]) )\n",
    "    capitalization_pattern += str(is_first_letter_in_word_capitalized (token_in_window[4]) )\n",
    "    \n",
    "    return {\n",
    "        'pre_word_1' : token_in_window[0],\n",
    "        'pre_word_2' : token_in_window[1],\n",
    "        'cur_word'   : token_in_window[2],\n",
    "        'post_word_1': token_in_window[3],\n",
    "        'post_word_2': token_in_window[4],\n",
    "        \n",
    "        'pre_entity_1':pre_entity_1, # this is extended BIO tag\n",
    "        'pre_entity_2':pre_entity_2,\n",
    "        \n",
    "        'pos_tag_1': pos_tags[0],\n",
    "        'pos_tag_2': pos_tags[1], \n",
    "        'pos_tag_3': pos_tags[2],\n",
    "        'pos_tag_4': pos_tags[3], \n",
    "        'pos_tag_5': pos_tags[4],\n",
    "        \n",
    "        'bio_tag_1': iob_tags[0],\n",
    "        'bio_tag_2': iob_tags[1],\n",
    "        'bio_tag_3': iob_tags[2], \n",
    "        'bio_tag_4': iob_tags[3],\n",
    "        'bio_tag_5': iob_tags[4], \n",
    "        \n",
    "        'fw': is_first_word_in_sentence,\n",
    "        'lw': is_last_word_in_sentence,\n",
    "        'acap': is_all_capitalized,\n",
    "        'fcap': is_first_letter_capitalized,\n",
    "        'cap_ptrn':capitalization_pattern\n",
    "       }       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We need to take following out from the input sentence\n",
    "    VB, VBD,VBG,VBZ, CC, CD, IN, NN, DT, NNS, PRP$,RB, JJ, RP, TO,PRP\n",
    "even before supplying to classifier. \n",
    "\n",
    "This is needed becuase else we will end up in creating imbalanced data set with 100 time more entries for OTHER categories\n",
    "\n",
    "Advance concept\n",
    "===============\n",
    "    It is mostly NNP for all entities but\n",
    "\n",
    "    JJ and NNPS -  come in I-PER \n",
    "    JJ , 's , DT followed by JJ ( The Oval) - come in I-LOC \n",
    "\n",
    "    For I-ORG , Syntactic features seems important\n",
    "        i.\tDependency path\n",
    "        ii.\tIOB chain\n",
    "    Like The Test and County Criket Board, \n",
    "'''\n",
    "def include_for_others(sentence,IOB_index):\n",
    "    return 1\n",
    "    #took VBD I-VP O\n",
    "    #Friday NNP I-NP O\n",
    "    if sentence[IOB_index-2] == 'NNP' or sentence[IOB_index-2] == 'IN':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8074\n"
     ]
    }
   ],
   "source": [
    "featuresets =[]\n",
    "'''\n",
    "The CONLL format is \n",
    "LEICESTERSHIRE NNP I-NP I-ORG\n",
    "i.e \n",
    "<word> <POStag> <IOB tag> <IOB tag for Organizayion>\n",
    "\n",
    "Hence i am parsing the sentence in similar way to extract features\n",
    "'''\n",
    "sentenceFeatures = []\n",
    "tags = []\n",
    "for sentence in sentences_with_words:\n",
    "    featuresets = []\n",
    "    sent_len = len(sentence) \n",
    "    num_word = (sent_len-1)/4\n",
    "\n",
    "    if(num_word < 3): #in some case the sentence is of just 1 or 2 words. I am avoiding those sentences for now\n",
    "        continue\n",
    "    count = 0\n",
    "    while (count < num_word):\n",
    "        IOB_index = (count * 4 )+ 3\n",
    "        if ( sentence[IOB_index] == 'I-ORG'):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'ORG') )\n",
    "        elif ( sentence[IOB_index] == 'I-LOC'):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'LOC') )\n",
    "        elif ( sentence[IOB_index] == 'I-PER'):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'PER') )\n",
    "        elif ( sentence[IOB_index] == 'I-MISC'):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'MISC') )\n",
    "        elif include_for_others(sentence,IOB_index):\n",
    "            featuresets.append( (extract_features(sentence,IOB_index) , 'OTHER') )\n",
    " \n",
    "        count = count + 1\n",
    "    sentenceFeatures.append(featuresets)\n",
    "    \n",
    "print len(sentenceFeatures)\n",
    "#print featuresets\n",
    "#Sample feature set should be list of tuples which should look like \n",
    "#[({'ed': 4, 'rel_type': 'PL', 'ps2': 0, 'ps1': 0, 'e1': 'Person', 'e2': 'Location'}, 'positive'), \n",
    "#({'ed': 1, 'rel_type': 'CL', 'ps2': 0, 'ps1': 0, 'e1': 'Company', 'e2': 'Location'}, 'positive'), \n",
    "#({'ed': 2, 'rel_type': 'PL', 'ps2': 0, 'ps1': 1, 'e1': 'Person', 'e2': 'Location'}, 'negative')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8074\n",
      "8074\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#take the complete train file and make train set\n",
    "number_of_sample_to_take = len(sentenceFeatures)\n",
    "print number_of_sample_to_take\n",
    "partition = 1\n",
    "\n",
    "train_set, test_set = sentenceFeatures[0:number_of_sample_to_take/partition], featuresets[number_of_sample_to_take/partition + 1:]\n",
    "\n",
    "print len (train_set)\n",
    "print len (test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'fcap': 0, 'pre_entity_2': 'NA', 'pre_entity_1': 'NA', 'acap': 1, 'cap_ptrn': '00001', 'pre_word_1': 'NA', 'lw': 0, 'pre_word_2': 'NA', 'cur_word': 'EU', 'fw': 1, 'pos_tag_5': 'JJ', 'pos_tag_4': 'VBZ', 'pos_tag_3': 'NNP', 'pos_tag_2': 'NA', 'pos_tag_1': 'NA', 'bio_tag_3': 'I-NP', 'bio_tag_2': 'NA', 'bio_tag_1': 'NA', 'bio_tag_5': 'I-NP', 'bio_tag_4': 'I-VP', 'post_word_2': 'German', 'post_word_1': 'rejects'}, 'ORG'), ({'fcap': 0, 'pre_entity_2': 'NA', 'pre_entity_1': 'I-ORG', 'acap': 0, 'cap_ptrn': '00010', 'pre_word_1': 'NA', 'lw': 0, 'pre_word_2': 'EU', 'cur_word': 'rejects', 'fw': 0, 'pos_tag_5': 'NN', 'pos_tag_4': 'JJ', 'pos_tag_3': 'VBZ', 'pos_tag_2': 'NNP', 'pos_tag_1': 'I-NP', 'bio_tag_3': 'I-VP', 'bio_tag_2': 'I-NP', 'bio_tag_1': 'O', 'bio_tag_5': 'I-NP', 'bio_tag_4': 'I-NP', 'post_word_2': 'call', 'post_word_1': 'German'}, 'OTHER'), ({'fcap': 1, 'pre_entity_2': 'I-ORG', 'pre_entity_1': 'O', 'acap': 0, 'cap_ptrn': '00100', 'pre_word_1': 'EU', 'lw': 0, 'pre_word_2': 'rejects', 'cur_word': 'German', 'fw': 0, 'pos_tag_5': 'TO', 'pos_tag_4': 'NN', 'pos_tag_3': 'JJ', 'pos_tag_2': 'VBZ', 'pos_tag_1': 'NNP', 'bio_tag_3': 'I-NP', 'bio_tag_2': 'I-VP', 'bio_tag_1': 'I-NP', 'bio_tag_5': 'I-VP', 'bio_tag_4': 'I-NP', 'post_word_2': 'to', 'post_word_1': 'call'}, 'MISC'), ({'fcap': 0, 'pre_entity_2': 'O', 'pre_entity_1': 'I-MISC', 'acap': 0, 'cap_ptrn': '01000', 'pre_word_1': 'rejects', 'lw': 0, 'pre_word_2': 'German', 'cur_word': 'call', 'fw': 0, 'pos_tag_5': 'VB', 'pos_tag_4': 'TO', 'pos_tag_3': 'NN', 'pos_tag_2': 'JJ', 'pos_tag_1': 'VBZ', 'bio_tag_3': 'I-NP', 'bio_tag_2': 'I-NP', 'bio_tag_1': 'I-VP', 'bio_tag_5': 'I-VP', 'bio_tag_4': 'I-VP', 'post_word_2': 'boycott', 'post_word_1': 'to'}, 'OTHER'), ({'fcap': 0, 'pre_entity_2': 'I-MISC', 'pre_entity_1': 'O', 'acap': 0, 'cap_ptrn': '10001', 'pre_word_1': 'German', 'lw': 0, 'pre_word_2': 'call', 'cur_word': 'to', 'fw': 0, 'pos_tag_5': 'JJ', 'pos_tag_4': 'VB', 'pos_tag_3': 'TO', 'pos_tag_2': 'NN', 'pos_tag_1': 'JJ', 'bio_tag_3': 'I-VP', 'bio_tag_2': 'I-NP', 'bio_tag_1': 'I-NP', 'bio_tag_5': 'I-NP', 'bio_tag_4': 'I-VP', 'post_word_2': 'British', 'post_word_1': 'boycott'}, 'OTHER'), ({'fcap': 0, 'pre_entity_2': 'O', 'pre_entity_1': 'O', 'acap': 0, 'cap_ptrn': '00010', 'pre_word_1': 'call', 'lw': 0, 'pre_word_2': 'to', 'cur_word': 'boycott', 'fw': 0, 'pos_tag_5': 'NN', 'pos_tag_4': 'JJ', 'pos_tag_3': 'VB', 'pos_tag_2': 'TO', 'pos_tag_1': 'NN', 'bio_tag_3': 'I-VP', 'bio_tag_2': 'I-VP', 'bio_tag_1': 'I-NP', 'bio_tag_5': 'I-NP', 'bio_tag_4': 'I-NP', 'post_word_2': 'lamb', 'post_word_1': 'British'}, 'OTHER'), ({'fcap': 1, 'pre_entity_2': 'O', 'pre_entity_1': 'O', 'acap': 0, 'cap_ptrn': '00100', 'pre_word_1': 'to', 'lw': 0, 'pre_word_2': 'boycott', 'cur_word': 'British', 'fw': 0, 'pos_tag_5': 'NA', 'pos_tag_4': 'NN', 'pos_tag_3': 'JJ', 'pos_tag_2': 'VB', 'pos_tag_1': 'TO', 'bio_tag_3': 'I-NP', 'bio_tag_2': 'I-VP', 'bio_tag_1': 'I-VP', 'bio_tag_5': 'NA', 'bio_tag_4': 'I-NP', 'post_word_2': 'NA', 'post_word_1': 'lamb'}, 'MISC'), ({'fcap': 0, 'pre_entity_2': 'O', 'pre_entity_1': 'I-MISC', 'acap': 0, 'cap_ptrn': '01000', 'pre_word_1': 'boycott', 'lw': 1, 'pre_word_2': 'British', 'cur_word': 'lamb', 'fw': 0, 'pos_tag_5': 'NA', 'pos_tag_4': 'NA', 'pos_tag_3': 'NN', 'pos_tag_2': 'JJ', 'pos_tag_1': 'VB', 'bio_tag_3': 'I-NP', 'bio_tag_2': 'I-NP', 'bio_tag_1': 'I-VP', 'bio_tag_5': 'NA', 'bio_tag_4': 'NA', 'post_word_2': 'NA', 'post_word_1': 'NA'}, 'OTHER')]\n"
     ]
    }
   ],
   "source": [
    "# convert the train set into a format that is expected by CRF library\n",
    "X_train=[[]]\n",
    "Y_train = [[]]\n",
    "\n",
    "length = len(train_set)\n",
    "print train_set[0]\n",
    "for sent in train_set[0:length]:\n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    for word in sent:\n",
    "        temp1.append(word[0])\n",
    "        temp2.append(word[1])\n",
    "    X_train.append(temp1)\n",
    "    Y_train.append(temp2)\n",
    "    \n",
    "    \n",
    "    temp1 = []\n",
    "    temp2 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the classfier\n",
    "%%time\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(X_train, Y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "\n",
    "print xseq\n",
    "print yseq\n",
    "\n",
    "trainer.train('D:\\\\pb_anshul\\\\temp\\\\temp_prj\\\\nlp_prj\\\\NER\\\\CRF\\\\conllCRF.crfsuite')\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "trainer.logparser.last_iteration\n",
    "print len(trainer.logparser.iterations), trainer.logparser.iterations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### done "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "number_of_sample_to_take_crf = len(sentenceFeatures)\n",
    "partition = 2\n",
    "\n",
    "print len(sentenceFeatures[0])\n",
    "crftrain_set, crftest_set = sentenceFeatures[0:number_of_sample_to_take_crf/partition], sentenceFeatures[number_of_sample_to_take_crf/partition + 1:]\n",
    "print len(sentenceFeatures)\n",
    "print len(crftrain_set)\n",
    "\n",
    "\n",
    "X_train=[[]]\n",
    "Y_train = [[]]\n",
    "\n",
    "length = len(crftrain_set)\n",
    "print crftrain_set[0]\n",
    "for sent in crftrain_set[0:length]:\n",
    "    #print sent\n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    for word in sent:\n",
    "        temp1.append(word[0])\n",
    "        temp2.append(word[1])\n",
    "    X_train.append(temp1)\n",
    "    Y_train.append(temp2)\n",
    "    \n",
    "    \n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "\n",
    "print \"\"\n",
    "print X_train[1]\n",
    "print \"\"\n",
    "print Y_train[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=[[]]\n",
    "Y_test = [[]]\n",
    "\n",
    "lengthtest = len(crftest_set)\n",
    "print lengthtest\n",
    "\n",
    "for sent in crftest_set[0:lengthtest]:\n",
    "    #print sent\n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    for word in sent:\n",
    "        temp1.append(word[0])\n",
    "        temp2.append(word[1])\n",
    "    X_test.append(temp1)\n",
    "    Y_test.append(temp2)\n",
    "    \n",
    "    temp1 = []\n",
    "    temp2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('conll2002-esp.crfsuite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print tagger.tag(X_test[1])\n",
    "print Y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "Y_pred=[]\n",
    "for row in X_test:\n",
    "    Y_pred.append(tagger.tag(row))\n",
    "print len(Y_pred)\n",
    "new_y_Pred=[]\n",
    "new_y_Test=[]\n",
    "for row in Y_test:\n",
    "    new_y_Test=new_y_Test+row\n",
    "\n",
    "for row in Y_pred:\n",
    "    new_y_Pred=new_y_Pred+row\n",
    "print len(new_y_Test)\n",
    "#Y_pred = tagger.tag(X_test[1])\n",
    "cm = ConfusionMatrix(new_y_Test, new_y_Pred)\n",
    "print cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "true_positives = Counter()\n",
    "false_negatives = Counter()\n",
    "false_positives = Counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = set('LOC MISC ORG OTHER PER'.split())\n",
    "for i in labels:\n",
    "    for j in labels:\n",
    "        if i == j:\n",
    "            true_positives[i] += cm[i,j]\n",
    "        else:\n",
    "            false_negatives[i] += cm[i,j]\n",
    "            false_positives[j] += cm[i,j]\n",
    "\n",
    "print \"TP:\", sum(true_positives.values()), true_positives\n",
    "print \"FN:\", sum(false_negatives.values()), false_negatives\n",
    "print \"FP:\", sum(false_positives.values()), false_positives\n",
    "\n",
    "print \"Accuracy\", float(sum(true_positives.values()))*100/(sum(true_positives.values()) + sum(false_negatives.values()) + sum(false_positives.values()))\n",
    "print \n",
    "\n",
    "print \"Fscore for each label\"\n",
    "for i in sorted(labels):\n",
    "    if true_positives[i] == 0:\n",
    "        fscore = 0\n",
    "    else:\n",
    "        precision = true_positives[i] / float(true_positives[i]+false_positives[i])\n",
    "        recall = true_positives[i] / float(true_positives[i]+false_negatives[i])\n",
    "        fscore = 2 * (precision * recall) / float(precision + recall)\n",
    "    print i, fscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
